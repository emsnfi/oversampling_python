{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0035f47e215fd5964214b6dc5985656f5ef64de389d25889fda4816acfc63ee4e",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "以下為正式的最佳化分類運作"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 引入資料\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'glass5-5-fold'#'abalone19-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' # pima-5-fold yeast-2_vs_8-5-fold\n",
    "\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 計算要補多少值\n",
    "def find_maj(sample_class): # 給 class 資料\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    #print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "     #   print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群的群中心 Elbow\n",
    "# polynom_fit_SMOTE\n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "\n",
    "centerpolynom = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.polynom_fit_SMOTE() # 產生數據\n",
    "    \n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    #print(Counter(y_polynom)) # smote 後的狀況\n",
    "    #newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) \n",
    "    \n",
    "    overpolynom.append(alloverpolynom)\n",
    "    tempcenterpolynom=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1]);\n",
    "        print(\"原本\",origincount)\n",
    "        countfor = math.floor(int(classCount[i][1])*0.8); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overpolynom[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            #print(\"要產生\",countfor)\n",
    "            tempcenterpolynom=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                temppolynom = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "              \n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                temppolynom.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(temppolynom))\n",
    "            #tempcenterpolynom.extend(temppolynom) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterpolynom = tempcenterpolynom+temppolynom\n",
    "                \n",
    "            centerpolynom.append(tempcenterpolynom) # 所有資料集所選到的資料\n",
    "            #print(\"真的有幾筆\",len(centerpolynom[ii]))\n",
    "\n",
    "#print(len(centerpolynom[0])) # 第一份資料中的群中心數量 位置"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "centerpolynomvalue =[]\n",
    "for tr in train: # 不同份資料\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerpolynom)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerpolynom[i])):\n",
    "            indexpolynom = centerpolynom[i][j][0] + originlen #原始資料數量後面接的是新生成的資料\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overpolynom[i].iloc[indexpolynom]))\n",
    "        centerpolynomvalue.append(alltemp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群的群中心\n",
    "# ProWSyn\n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "\n",
    "centerProWSyn = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    \n",
    "    #print(\"各類別差距\",classCount)\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.ProWSyn() # 產生數據\n",
    "    \n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    #print(Counter(y_ProWSyn)) # smote 後的狀況\n",
    "    #newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    \n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) \n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "    tempcenterProWSyn=[]\n",
    "    \n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1])\n",
    "        print(\"原本\",origincount)\n",
    "        origincount = int(classCount[i][1])\n",
    "        print(\"原本\",origincount)\n",
    "        countfor = math.floor(int(classCount[i][1])*0.8); # 要產生多\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overProWSyn[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            \n",
    "            tempcenterProWSyn=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                tempProWSyn = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "                #print(\"要產生\",countfor,\"比例\", labelRatio[ic])\n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                \n",
    "                #print(\"各群產生：\",countforlabel)\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                tempProWSyn.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(tempProWSyn))\n",
    "            #tempcenterProWSyn.extend(tempProWSyn) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterProWSyn = tempcenterProWSyn+tempProWSyn\n",
    "                #if(ct==cluster_count):\n",
    "                 #   print(\"該份資料集所要的所有資料\",len(tempcenterProWSyn))\n",
    "                \n",
    "            centerProWSyn.append(tempcenterProWSyn) # 所有資料集所選到的資料\n",
    "            print(\"真的有幾筆\",len(centerProWSyn[ii]))\n",
    "\n",
    "#print(len(centerProWSyn[0])) # 第一份資料中的群中心數量 位置"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "centerProWSynvalue =[]\n",
    "for tr in train: # 不同份資料\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerProWSyn)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerProWSyn[i])):\n",
    "            indexProWSyn = centerProWSyn[i][j][0] + originlen #原始資料數量後面接的是新生成的資料\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overProWSyn[i].iloc[indexProWSyn]))\n",
    "        centerProWSynvalue.append(alltemp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 合併 小類 center polynom + ProWSyn \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerProWSynvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群的群中心 \n",
    "# SMOTEIPF_fit_SMOTE\n",
    "alloverSMOTEIPF = []\n",
    "overSMOTEIPF = []\n",
    "\n",
    "centerSMOTEIPF = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.SMOTE_IPF() # 產生數據\n",
    "    \n",
    "    X_SMOTEIPF,y_SMOTEIPF = over.sample(finaldata,output)\n",
    "    print(Counter(y_SMOTEIPF)) # smote 後的狀況\n",
    "    #newDataCount = len(X_SMOTEIPF) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_SMOTEIPF 跟 y_SMOTEIPF 和在一起\n",
    "    X_SMOTEIPF = pd.DataFrame(X_SMOTEIPF)\n",
    "    y_SMOTEIPF = pd.DataFrame(y_SMOTEIPF)\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) \n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "    tempcenterSMOTEIPF=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1])\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) \n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "    tempcenterSMOTEIPF=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1])\n",
    "        countfor = math.floor(int(classCount[i][1])*0.2); # 要產生多少數據  無條件捨去\n",
    "\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overSMOTEIPF[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            #print(\"要產生\",countfor)\n",
    "            tempcenterSMOTEIPF=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                tempSMOTEIPF = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "              \n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                tempSMOTEIPF.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(tempSMOTEIPF))\n",
    "            #tempcenterSMOTEIPF.extend(tempSMOTEIPF) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterSMOTEIPF = tempcenterSMOTEIPF+tempSMOTEIPF\n",
    "                if(ct==cluster_count):\n",
    "                    print(\"該份資料集所要的所有資料\",len(tempcenterSMOTEIPF))\n",
    "                \n",
    "            centerSMOTEIPF.append(tempcenterSMOTEIPF) # 所有資料集所選到的資料\n",
    "            print(\"真的有幾筆\",len(centerSMOTEIPF[ii]))\n",
    "\n",
    "#print(len(centerSMOTEIPF[0])) # 第一份資料中的群中心數量 位置"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# centerSMOTEIPF 只是 index ，value 是取出值\n",
    "centerSMOTEIPFvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerSMOTEIPF)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerSMOTEIPF[i])):\n",
    "            indexSMOTEIPF = centerSMOTEIPF[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overSMOTEIPF[i].iloc[indexSMOTEIPF]))\n",
    "        centerSMOTEIPFvalue.append(alltemp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 合併 小類 center polynom-fit-SMOTE + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 合併 小類 center ProWSyn  + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# polynom ProWSyn SMOTE_IPF 三個合併\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = temp + centerpolynomvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "accuracies =[]\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    \n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}