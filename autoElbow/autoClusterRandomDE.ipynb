{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "440c858d28a36c9981deb3f0b3542b13a925970b1503e694b09cf153c81eca91"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "以下為正式的最佳化分類運作 Random 選取各群一定比例的數據"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 引入資料\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'glass5-5-fold'#'abalone19-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' # pima-5-fold yeast-2_vs_8-5-fold\n",
    "\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 計算要補多少值\n",
    "def find_maj(sample_class): # 給 class 資料\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    #print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "     #   print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_excel(train[0],index_col=0)\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群 random 選資料\n",
    "# polynom_fit_SMOTE\n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "\n",
    "randompolynom =[]\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.polynom_fit_SMOTE() # 產生數據\n",
    "    \n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    #print(Counter(y_polynom)) # smote 後的狀況\n",
    "    #newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) \n",
    "    \n",
    "    overpolynom.append(alloverpolynom)\n",
    "    #print(overpolynom[ii])\n",
    "\n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1]); #原本的數據量\n",
    "       \n",
    "        countfor = math.floor(int(classCount[i][1])*0.4); # 要產生多少數據  無條件捨去\n",
    "        \n",
    "        \n",
    "        if(countfor>0):\n",
    "            print(\"原本\",origincount)\n",
    "            dtemp = pd.DataFrame(overpolynom[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類 # 把最後的類別拿掉\n",
    "            X_ = dtemp.iloc[originlen:,-1] # 拿掉的分類 雖然都一樣\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            X_.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            \n",
    "            df = pd.concat([X,X_,klabel],axis=1)\n",
    "            X = X.astype('float64')    \n",
    "            # random 挑選各群的資料\n",
    "        \n",
    "            ct = 0 \n",
    "            randomvaluetemp = [] # 放不同切分資料集的值\n",
    "            \n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1\n",
    "                randomIndex = []\n",
    "                randomtemp = []\n",
    "                #temppolynom = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic])    \n",
    "                \n",
    "                tempdf.reset_index(drop=True, inplace=True)\n",
    "                # 不同群的random\n",
    "                randomIndex.extend([random.randint(0,len(tempdf)-1) for _ in range(countforlabel)]) #該群的index\n",
    "\n",
    "                # 該群真實的值 index 是位置\n",
    "                for index in randomIndex:\n",
    "                    randomtemp.append(tempdf.iloc[index,:-1])\n",
    "\n",
    "                randomvaluetemp.extend(randomtemp) # 一個切分資料集 所有群的資料\n",
    "            print(\"countfor\",countfor)\n",
    "            print(\"長度\",len(randomvaluetemp))\n",
    "                \n",
    "            randompolynom.append(randomvaluetemp) # 所有資料集要取的資料"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群 random 選資料\n",
    "# ProWSyn\n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "\n",
    "randomProWSyn =[]\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.ProWSyn() # 產生數據\n",
    "    \n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    #print(Counter(y_ProWSyn)) # smote 後的狀況\n",
    "    #newDataCount = len(X_ProWSyn) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_ProWSyn 跟 y_ProWSyn 和在一起\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) \n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "    #print(overProWSyn[ii])\n",
    "\n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1]); #原本的數據量\n",
    "       \n",
    "  \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "    #print(overProWSyn[ii])\n",
    "\n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1]); #原本的數據量\n",
    "       \n",
    "        countfor = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        \n",
    "        \n",
    "        if(countfor>0):\n",
    "            print(\"原本\",origincount)\n",
    "            dtemp = pd.DataFrame(overProWSyn[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類 # 把最後的類別拿掉\n",
    "            X_ = dtemp.iloc[originlen:,-1] # 拿掉的分類 雖然都一樣\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            X_.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            \n",
    "            df = pd.concat([X,X_,klabel],axis=1)\n",
    "            X = X.astype('float64')    \n",
    "            # random 挑選各群的資料\n",
    "        \n",
    "            ct = 0 \n",
    "            randomvaluetemp = [] # 放不同切分資料集的值\n",
    "            \n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1\n",
    "                randomIndex = []\n",
    "                randomtemp = []\n",
    "                #tempProWSyn = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic])    \n",
    "                \n",
    "                tempdf.reset_index(drop=True, inplace=True)\n",
    "                # 不同群的random\n",
    "                randomIndex.extend([random.randint(0,len(tempdf)-1) for _ in range(countforlabel)]) #該群的index\n",
    "\n",
    "                # 該群真實的值 index 是位置\n",
    "                for index in randomIndex:\n",
    "                    randomtemp.append(tempdf.iloc[index,:-1])\n",
    "\n",
    "                randomvaluetemp.extend(randomtemp) # 一個切分資料集 所有群的資料\n",
    "            print(\"countfor\",countfor)\n",
    "            print(\"長度\",len(randomvaluetemp))\n",
    "                \n",
    "            randomProWSyn.append(randomvaluetemp) # 所有資料集要取的資料"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# polynom  ProWSyn\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randomProWSyn)):\n",
    "    temp = randompolynom[i] + randomProWSyn[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(allRandomHalf[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster 最佳化群數後 各群 random 選資料\n",
    "# SMOTEIPF\n",
    "alloverSMOTEIPF = []\n",
    "overSMOTEIPF = []\n",
    "\n",
    "randomSMOTEIPF =[]\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.SMOTE_IPF() # 產生數據\n",
    "    \n",
    "    X_SMOTEIPF,y_SMOTEIPF = over.sample(finaldata,output)\n",
    "    #print(Counter(y_SMOTEIPF)) # smote 後的狀況\n",
    "    #newDataCount = len(X_SMOTEIPF) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_SMOTEIPF 跟 y_SMOTEIPF 和在一起\n",
    "    X_SMOTEIPF = pd.DataFrame(X_SMOTEIPF)\n",
    "    y_SMOTEIPF = pd.DataFrame(y_SMOTEIPF)\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) \n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "    #print(overSMOTEIPF[ii])\n",
    "\n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        origincount = int(classCount[i][1]); #原本的數據量\n",
    "       \n",
    "        countfor = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        \n",
    "        \n",
    "        if(countfor>0):\n",
    "            print(\"原本\",origincount)\n",
    "            dtemp = pd.DataFrame(overSMOTEIPF[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類 # 把最後的類別拿掉\n",
    "            X_ = dtemp.iloc[originlen:,-1] # 拿掉的分類 雖然都一樣\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            X_.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/origincount)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            \n",
    "            df = pd.concat([X,X_,klabel],axis=1)\n",
    "            X = X.astype('float64')    \n",
    "            # random 挑選各群的資料\n",
    "        \n",
    "            ct = 0 \n",
    "            randomvaluetemp = [] # 放不同切分資料集的值\n",
    "            \n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1\n",
    "                randomIndex = []\n",
    "                randomtemp = []\n",
    "                #tempSMOTEIPF = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic])    \n",
    "                \n",
    "                tempdf.reset_index(drop=True, inplace=True)\n",
    "                # 不同群的random\n",
    "                randomIndex.extend([random.randint(0,len(tempdf)-1) for _ in range(countforlabel)]) #該群的index\n",
    "\n",
    "                # 該群真實的值 index 是位置\n",
    "                for index in randomIndex:\n",
    "                    randomtemp.append(tempdf.iloc[index,:-1])\n",
    "\n",
    "                randomvaluetemp.extend(randomtemp) # 一個切分資料集 所有群的資料\n",
    "            print(\"countfor\",countfor)\n",
    "            print(\"長度\",len(randomvaluetemp))\n",
    "                \n",
    "            randomSMOTEIPF.append(randomvaluetemp) # 所有資料集要取的資料"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# polynom SMOTE_IPF\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randompolynom)):\n",
    "    temp = randompolynom[i] + randomSMOTEIPF[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ProWSyn SMOTE_IPF\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randomProWSyn)):\n",
    "    temp = randomSMOTEIPF[i] + randomProWSyn[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# polynom ProWSyn SMOTE_IPF 三個合併\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randomProWSyn)):\n",
    "    temp = randomSMOTEIPF[i] + randomProWSyn[i] # list 合併\n",
    "    temp = temp + randompolynom[i]\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "accuracies =[]\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "   \n",
    "    mergeRandom = pd.concat([data,allRandomHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}