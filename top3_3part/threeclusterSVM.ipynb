{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0440c858d28a36c9981deb3f0b3542b13a925970b1503e694b09cf153c81eca91",
   "display_name": "Python 3.7.7  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "440c858d28a36c9981deb3f0b3542b13a925970b1503e694b09cf153c81eca91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'yeast1-5-fold'#'abalone19-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' # pima-5-fold yeast-2_vs_8-5-fold glass5-5-fold segment0-5-fold yeast-5-fold yeast1-5-fold\n",
    "\n",
    "\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = pd.read_excel(test[0])\n",
    "dd = pd.DataFrame(fil)\n",
    "s = dd.columns[-1]\n",
    "dd[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 純 polynom_fit_SMOTE\n",
    "\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "accuracies=[]\n",
    "#print(os.getcwd())\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    \n",
    "    finaldata = data.iloc[:,:l]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    output = le.fit_transform(output)\n",
    "    \n",
    "    tempover = []\n",
    "    \n",
    "    #over = SMOTE()  # SMOTE\n",
    "    #over = sv.polynom_fit_SMOTE() #  polynom_fit_SMOTE\n",
    "    #over = sv.SMOTE_IPF() # SMOTE_IPF()\n",
    "    over = sv.ProWSyn() # ProWSyn()\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    #X_polynom,y_polynom = over.fit_resample(finaldata,output)\n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "  \n",
    "    clf=svm.SVC(kernel='rbf',C=1,gamma='auto')\n",
    "    clf = clf.fit(X_polynom,y_polynom)\n",
    "    \n",
    "    #clf = clf.fit(finaldata,output)\n",
    "    #newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    \n",
    "   \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data[lastColumn]= test_data[lastColumn].str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    \"\"\"\n",
    "    # le = preprocessing.LabelEncoder()\n",
    "    # for i in range(test_data.shape[1]):\n",
    "    #     test_data.iloc[:,i] = le.fit_transform(test_data.iloc[:,i]) \n",
    "    \"\"\"\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] # 劃分\n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    \n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "\n",
    "print(mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Cluster "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-31 10:21:57,588:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:21:58,149:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:21:58,717:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:21:59,325:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:21:59,906:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "344\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Cluster\n",
    "# polynom_fit_SMOTE\n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "\n",
    "centerpolynom = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.polynom_fit_SMOTE()\n",
    "    \n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overpolynom.append(alloverpolynom)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.4); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overpolynom[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerpolynom.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerpolynom[0])) # 第一份資料中的群中心數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerpolynom 只是 index ，value 是取出值\n",
    "centerpolynomvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerpolynom)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerpolynom[i])):\n",
    "            indexpolynom = centerpolynom[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overpolynom[i].iloc[indexpolynom]))\n",
    "        centerpolynomvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-31 10:22:02,901:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "origin Counter({0: 844, 1: 343})\n",
      "Counter({0: 844, 1: 844})\n",
      "2021-05-31 10:22:03,685:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "origin Counter({0: 844, 1: 343})\n",
      "Counter({0: 844, 1: 844})\n",
      "2021-05-31 10:22:04,409:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "origin Counter({0: 844, 1: 343})\n",
      "Counter({0: 844, 1: 844})\n",
      "2021-05-31 10:22:05,213:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "origin Counter({0: 844, 1: 343})\n",
      "Counter({0: 844, 1: 844})\n",
      "2021-05-31 10:22:05,910:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "844\n",
      "344\n",
      "origin Counter({0: 844, 1: 344})\n",
      "Counter({0: 844, 1: 844})\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Cluster\n",
    "# ProWSyn\n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "\n",
    "centerProWSyn = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    print(\"origin\",Counter(output))\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.ProWSyn()\n",
    "    \n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    print(Counter(y_ProWSyn))\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overProWSyn[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerProWSyn.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerProWSyn[0])) # 第一份資料中的群中心數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerProWSyn\n",
    "centerProWSynvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerProWSyn)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerProWSyn[i])):\n",
    "            indexProWSyn = centerProWSyn[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overProWSyn[i].iloc[indexProWSyn]))\n",
    "        centerProWSynvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom + ProWSyn \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerProWSynvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-31 10:22:08,653:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 10:22:08,654:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 10:22:08,726:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:08,785:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:08,842:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:22:09,580:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 10:22:09,581:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 10:22:09,639:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:09,694:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:09,750:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:22:10,429:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 10:22:10,430:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 10:22:10,489:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:10,545:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:10,605:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:22:11,288:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 10:22:11,289:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 10:22:11,349:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:11,402:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:11,456:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "844\n",
      "343\n",
      "2021-05-31 10:22:12,117:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 10:22:12,118:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 10:22:12,185:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:12,241:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 10:22:12,295:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "844\n",
      "344\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Cluster\n",
    "# SMOTE-IPF\n",
    "alloverSMOTEIPF = []\n",
    "overSMOTEIPF = []\n",
    "\n",
    "centerSMOTEIPF = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "  #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.SMOTE_IPF()\n",
    "    \n",
    "    X_SMOTEIPF,y_SMOTEIPF = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_SMOTEIPF = pd.DataFrame(X_SMOTEIPF)\n",
    "    y_SMOTEIPF = pd.DataFrame(y_SMOTEIPF)\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overSMOTEIPF[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerSMOTEIPF.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerSMOTEIPF[0])) # 第一份資料中的群中心數量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSMOTEIPF 只是 index ，value 是取出值\n",
    "centerSMOTEIPFvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerSMOTEIPF)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerSMOTEIPF[i])):\n",
    "            indexSMOTEIPF = centerSMOTEIPF[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overSMOTEIPF[i].iloc[indexSMOTEIPF]))\n",
    "        centerSMOTEIPFvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom-fit-SMOTE + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合併 小類 center ProWSyn  + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "execution_count": 1101
    }
   ],
   "source": [
    "# 合併 polynom ProWSyn SMOTE_IPF 三個\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = temp + centerpolynomvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCenterHalf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0.0: 844, 1.0: 843})\n",
      "Counter({0.0: 844, 1.0: 843})\n",
      "Counter({0.0: 844, 1.0: 843})\n",
      "Counter({0.0: 844, 1.0: 843})\n",
      "Counter({0.0: 844, 1.0: 844})\n",
      "0.5606873010418759\n"
     ]
    }
   ],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "accuracies = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    clf=svm.SVC(kernel='rbf',C=1,gamma='auto')\n",
    "\n",
    "    clf = clf.fit(finaldata,output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    \n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}