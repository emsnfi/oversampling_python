{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0035f47e215fd5964214b6dc5985656f5ef64de389d25889fda4816acfc63ee4e",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 elbow 的方式找出最好的分類\n",
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入資料\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'yeast-2_vs_8-5-fold'#'abalone19-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' # pima-5-fold yeast-2_vs_8-5-fold\n",
    "\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算要補多少值\n",
    "def find_maj(sample_class): # 給 class 資料\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    #print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "     #   print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(train[0],index_col=0)\n",
    "\n",
    "X = data.iloc[:,:data.shape[1]-1]\n",
    "y = data.iloc[:,-1]\n",
    "maj = classprocess(y)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X.reset_index(inplace=True)\n",
    "labels_true = y\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "allData= pd.concat([X,y],axis=1)\n",
    "allData = allData.drop(['index'],axis=1)\n",
    "allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(allData.iloc[:,-1])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method\n",
    "X = X.drop(['index'],axis=1)\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "w = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "cluster_count = w.elbow_value_ # 最佳要幾個點\n",
    "\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n",
    "c  = Counter(kmeans.labels_) # 各群的數量\n",
    "\n",
    "print(sorted(c))\n",
    "print(sorted(c.items()))\n",
    "# 總數除以分群數 \n",
    "# 該 label 數量 除以 全部數量 占比\n",
    "for key,element in sorted(c.items()):\n",
    "    print(key,\" \",element)\n",
    "    #print(element/y.shape[0]) # 不同群的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把分類標籤跟原始資料進行合併\n",
    "klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "\n",
    "df = pd.concat([allData,klabel],axis=1)\n",
    "# 各標籤分類的數據都存在 allCluster 內 裡面會依據\n",
    "allCluster = []\n",
    "for index in range(cluster_count):\n",
    "    allCluster.append(df[df['label']==index])\n",
    "    print(len(allCluster[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出各群群中心\n",
    "len(kmeans.labels_) # 分群的標籤\n",
    "#找出各群 群中心\n",
    "kmeans.cluster_centers_[1][0] # 群中心\n"
   ]
  },
  {
   "source": [
    "以下為正式的最佳化分類運作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入資料\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'yeast-2_vs_8-5-fold'#'abalone19-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' # pima-5-fold yeast-2_vs_8-5-fold\n",
    "\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算要補多少值\n",
    "def find_maj(sample_class): # 給 class 資料\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    #print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "     #   print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 最佳化群數後 各群的群中心\n",
    "# polynom_fit_SMOTE\n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "\n",
    "centerpolynom = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.polynom() # 產生數據\n",
    "    \n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    print(Counter(y_polynom)) # smote 後的狀況\n",
    "    #newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) \n",
    "    \n",
    "    overpolynom.append(alloverpolynom)\n",
    "    tempcenterpolynom=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        countfor = math.floor(int(classCount[i][1])*1); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overpolynom[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/countfor)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            #print(\"要產生\",countfor)\n",
    "            tempcenterpolynom=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                temppolynom = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "              \n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                temppolynom.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(temppolynom))\n",
    "            #tempcenterpolynom.extend(temppolynom) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterpolynom = tempcenterpolynom+temppolynom\n",
    "                if(ct==cluster_count):\n",
    "                    print(\"該份資料集所要的所有資料\",len(tempcenterpolynom))\n",
    "                \n",
    "            centerpolynom.append(tempcenterpolynom) # 所有資料集所選到的資料\n",
    "            print(\"真的有幾筆\",len(centerpolynom[ii]))\n",
    "\n",
    "#print(len(centerpolynom[0])) # 第一份資料中的群中心數量 位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 最佳化群數後 各群的群中心\n",
    "# ProWSyn\n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "\n",
    "centerProWSyn = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    #print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.ProWSyn() # 產生數據\n",
    "    \n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    #print(Counter(y_ProWSyn)) # smote 後的狀況\n",
    "    #newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) \n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "    tempcenterProWSyn=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        countfor = math.floor(int(classCount[i][1])*1); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overProWSyn[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            #print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/countfor)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            #print(\"要產生\",countfor)\n",
    "            tempcenterProWSyn=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                tempProWSyn = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "              \n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                tempProWSyn.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(tempProWSyn))\n",
    "            #tempcenterProWSyn.extend(tempProWSyn) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterProWSyn = tempcenterProWSyn+tempProWSyn\n",
    "                #if(ct==cluster_count):\n",
    "                 #   print(\"該份資料集所要的所有資料\",len(tempcenterProWSyn))\n",
    "                \n",
    "            centerProWSyn.append(tempcenterProWSyn) # 所有資料集所選到的資料\n",
    "            #print(\"真的有幾筆\",len(centerProWSyn[ii]))\n",
    "\n",
    "#print(len(centerProWSyn[0])) # 第一份資料中的群中心數量 位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom + ProWSyn \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerProWSynvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 最佳化群數後 各群的群中心 \n",
    "# SMOTEIPF_fit_SMOTE\n",
    "alloverSMOTEIPF = []\n",
    "overSMOTEIPF = []\n",
    "\n",
    "centerSMOTEIPF = []\n",
    "countfor = 0;\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    \n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0] # 原始資料的筆數\n",
    "    output = data.iloc[:,-1];\n",
    "    classCount = classprocess(output) # 各類別差距\n",
    "    finaldata = data.iloc[:,:-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    print(\"origin\",Counter(output)) # 原始的分類狀況\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.SMOTEIPF() # 產生數據\n",
    "    \n",
    "    X_SMOTEIPF,y_SMOTEIPF = over.sample(finaldata,output)\n",
    "    print(Counter(y_SMOTEIPF)) # smote 後的狀況\n",
    "    #newDataCount = len(X_SMOTEIPF) - len(data)  # 新生成的 data 數量\n",
    "\n",
    "    # 把 X_SMOTEIPF 跟 y_SMOTEIPF 和在一起\n",
    "    X_SMOTEIPF = pd.DataFrame(X_SMOTEIPF)\n",
    "    y_SMOTEIPF = pd.DataFrame(y_SMOTEIPF)\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) \n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "    tempcenterSMOTEIPF=[]\n",
    "   \n",
    "    for i in range(len(classCount)):# 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "        countfor = math.floor(int(classCount[i][1])*1); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            dtemp = pd.DataFrame(overSMOTEIPF[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的 都是小類\n",
    "            X.reset_index(inplace=True, drop=True)\n",
    "            print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "            model = KMeans()\n",
    "            visualizer = KElbowVisualizer(model, k=(1,12))\n",
    "\n",
    "            kmodel = visualizer.fit(X)        # Fit the data to the visualizer\n",
    "            cluster_count = kmodel.elbow_value_ # 最佳要分成幾群\n",
    "            kmeans = KMeans(n_clusters=cluster_count)\n",
    "            kmeans.fit(X)\n",
    "            label  = Counter(kmeans.labels_) # 標籤分類狀況\n",
    "\n",
    "           \n",
    "            #不同群的比例\n",
    "            labelRatio =[] \n",
    "            for key,element in sorted(label.items()):\n",
    "                labelRatio.append(element/countfor)\n",
    "            #print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "            klabel = pd.DataFrame({'label':kmeans.labels_}) # 建立一個欄位名為 label 的\n",
    "            df = pd.concat([X,klabel],axis=1) # X 是後來生成的數據 類別都是小類\n",
    "            #print(df)\n",
    "            centers = kmeans.cluster_centers_ # 各群群中心\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            \n",
    "            # 計算每個點跟各群中心的距離\n",
    "        \n",
    "            ct = 0 \n",
    "            #print(\"分成\",cluster_count,\"群\")\n",
    "            #print(\"要產生\",countfor)\n",
    "            tempcenterSMOTEIPF=[] # 清空\n",
    "            for ic in range(cluster_count):\n",
    "                ct+=1;\n",
    "                \n",
    "                tempSMOTEIPF = []\n",
    "                #把不同群過濾出來\n",
    "                tempdf =  df[df['label']==ic] # df 是 X 跟 label 結合後的 dataframe\n",
    "                #allCluster.append(df[df['label']==ic])\n",
    "               \n",
    "                \n",
    "                # 計算每個點跟群中心的距離\n",
    "                for i in range(tempdf.shape[0]-1): # 列 也就是幾筆資料\n",
    "                \n",
    "                    distance = []\n",
    "                    temp = 0; #放算出來的距離\n",
    "                    tempsum = 0;\n",
    "                    for j in range(tempdf.shape[1]-2):# 到前一欄 因為最後一欄為 label\n",
    "                        temp = pow((centers[ic][j]-tempdf.iloc[i][j]),2)  # 該欄位跟center欄位的距離\n",
    "                        tempsum = tempsum + temp\n",
    "                        #print(tempsum)\n",
    "                        tempindata[i] = tempsum \n",
    "                    \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "    \n",
    "                # 要按照比例挑出資料\n",
    "              \n",
    "                countforlabel = math.ceil(countfor * labelRatio[ic]) # 按照比例 給不同的數量 不同群不同數量\n",
    "                #print(\"比例\",labelRatio)\n",
    "                #print(\"count\",countforlabel)\n",
    "                \n",
    "                tempSMOTEIPF.extend(distancesortemp[:countforlabel]) #該群所要的數量\n",
    "                #print(\"該群所要的數量\",len(tempSMOTEIPF))\n",
    "            #tempcenterSMOTEIPF.extend(tempSMOTEIPF) # 該份資料集所要的所有資料\n",
    "                \n",
    "                #print(\"ct\",ct)\n",
    "                tempcenterSMOTEIPF = tempcenterSMOTEIPF+tempSMOTEIPF\n",
    "                if(ct==cluster_count):\n",
    "                    print(\"該份資料集所要的所有資料\",len(tempcenterSMOTEIPF))\n",
    "                \n",
    "            centerSMOTEIPF.append(tempcenterSMOTEIPF) # 所有資料集所選到的資料\n",
    "            print(\"真的有幾筆\",len(centerSMOTEIPF[ii]))\n",
    "\n",
    "#print(len(centerSMOTEIPF[0])) # 第一份資料中的群中心數量 位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerProWSyn[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerProWSynvalue =[]\n",
    "for tr in train: # 不同份資料\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerProWSyn)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerProWSyn[i])):\n",
    "            indexProWSyn = centerProWSyn[i][j][0] + originlen #原始資料數量後面接的是新生成的資料\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overProWSyn[i].iloc[indexProWSyn]))\n",
    "        centerProWSynvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom-fit-SMOTE + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合併 小類 center ProWSyn  + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "accuracies =[]\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    \n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  }
 ]
}