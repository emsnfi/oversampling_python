{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('3.7.7')",
   "metadata": {
    "interpreter": {
     "hash": "8a5478e73ab4ac786b177a0e1da34e6fd874585dfdc4463791ac8c0c5c0e6e05"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此篇要做 2 個以上的群中心\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import os;\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由於 RCSMOTE 出現問題 所以改用 boderline 加 SMOTE 加 borderline 加 ADASYN\n",
    "import os\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'haberman-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold' #'abalone19-5-fold' # \n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['haberman-5-2tra.xlsx',\n",
       " 'haberman-5-3tra.xlsx',\n",
       " 'haberman-5-1tra.xlsx',\n",
       " 'haberman-5-5tra.xlsx',\n",
       " 'haberman-5-4tra.xlsx']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "source": [
    "SMOTE 兩個群中心"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "180\n",
      "65\n",
      "180\n",
      "65\n",
      "180\n",
      "64\n",
      "180\n",
      "65\n",
      "180\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# SMOTE 群中心 index\n",
    "# 兩個 center\n",
    "from imblearn.over_sampling import  SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "centerSMOTE1 = []\n",
    "centerSMOTE2 = []\n",
    "countfor = 0;\n",
    "centerSMOTEall=[]\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverSMOTE = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    # 整理程式碼 參數使用可填入 多少個 cluster 可用 for 迴圈\n",
    "    for i in range(len(classCount)):\n",
    "        \n",
    "        countfor = math.floor(int(classCount[i][1])*0.5); # 全部需要的數量 無條件捨去\n",
    "        \n",
    "        countfor_center1 = math.ceil(countfor*0.3); # 第一個群中心的數量  佔全部的多少\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        countfor_center2 = int(countfor*0.7); # 第二個群中心的數量 \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=2)\n",
    "            dtemp = pd.DataFrame(overSMOTE[ii])\n",
    "            #dd = dtemp.iloc[originlen:,:]\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            #print(dd)\n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            #print(X)\n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for centerindex in range(len(centers)): \n",
    "                for i in range(X.shape[0]-1): # 列\n",
    "                    distance = []\n",
    "                    temp = 0;\n",
    "                    for j in range(X.shape[1]-1):#9 行\n",
    "                        temp = pow((centers[centerindex][j]-X.iloc[i][j]),2)  \n",
    "                        tempindata[i] = temp\n",
    "               \n",
    "                \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #distanceindex.append(distancesortemp[ii][0]+originlen)\n",
    "                \n",
    "                \n",
    "                if(centerindex == 0):\n",
    "                    #print(distancesortemp[:countfor_center1])\n",
    "                    #distancesortemp = distancesortemp[:countfor_center1]\n",
    "                    centerSMOTE1.append(distancesortemp[:countfor_center1])\n",
    "                elif(centerindex==1):\n",
    "                    centerSMOTE2.append(distancesortemp[:countfor_center2])\n",
    "        \n",
    "    \n",
    "    tempcenter = centerSMOTE1[ii] + centerSMOTE2[ii]\n",
    "\n",
    "    centerSMOTEall.append(tempcenter)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSMOTEmin 只是 index ，value 是取出值\n",
    "centerSMOTEvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerSMOTEall)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerSMOTEall[i])):\n",
    "            indexSMOTE = centerSMOTEall[i][j][0]+originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overSMOTE[i].iloc[indexSMOTE]))\n",
    "        centerSMOTEvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "180\n",
      "65\n",
      "180\n",
      "65\n",
      "180\n",
      "64\n",
      "180\n",
      "65\n",
      "180\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# ADASYN 群中心 兩個 center index\n",
    "\n",
    "from imblearn.over_sampling import  ADASYN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverADASYN = []\n",
    "overADASYN = []\n",
    "#randomADASYN = []\n",
    "centerADASYN1 = []\n",
    "centerADASYN2 = []\n",
    "countfor = 0;\n",
    "centerADASYNall=[]\n",
    "for ii,i in enumerate(train):\n",
    "    #randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    \n",
    "   \n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverADASYN = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overADASYN.append(alloverADASYN)\n",
    "    \n",
    "    # 整理程式碼 參數使用可填入 多少個 cluster 可用 for 迴圈\n",
    "    for i in range(len(classCount)):\n",
    "        \n",
    "        countfor = math.floor(int(classCount[i][1])*0.5); # 全部需要的數量 無條件捨去\n",
    "        \n",
    "        countfor_center1 = math.ceil(countfor*0.3); # 第一個群中心的數量  佔全部的多少\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        countfor_center2 = int(countfor*0.7); # 第二個群中心的數量 \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=2)\n",
    "            dtemp = pd.DataFrame(overADASYN[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for centerindex in range(len(centers)): \n",
    "                for i in range(X.shape[0]-1): # 列\n",
    "                    distance = []\n",
    "                    temp = 0;\n",
    "                    for j in range(X.shape[1]-1):#9 行\n",
    "                        temp = pow((centers[centerindex][j]-X.iloc[i][j]),2)  \n",
    "                        tempindata[i] = temp\n",
    "            \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "                if(centerindex == 0):\n",
    "                    #print(distancesortemp[:countfor_center1])\n",
    "                    centerADASYN1.append(distancesortemp[:countfor_center1])\n",
    "                elif(centerindex==1):\n",
    "                    centerADASYN2.append(distancesortemp[:countfor_center2])\n",
    "        \n",
    "\n",
    "    tempcenter = centerADASYN1[ii] + centerADASYN2[ii]\n",
    "\n",
    "    centerADASYNall.append(tempcenter)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerADASYN 只是 index ，value 是取出值\n",
    "centerADASYNvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerADASYNall)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerADASYNall[i])):\n",
    "            indexADASYN = centerADASYNall[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overADASYN[i].iloc[indexADASYN]))\n",
    "        centerADASYNvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 合併 小類 center ADASYN + SMOTE 各 \n",
    "allCenter = []\n",
    "temp = []\n",
    "for i in range(len(centerADASYNvalue)):\n",
    "    temp = centerADASYNvalue[i] + centerSMOTEvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenter.append(temp)\n",
    "\n",
    "len(allCenter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8011111111111111\n"
     ]
    }
   ],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "mergeRandom = []\n",
    "accuracies=[]\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    " \n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenter[index]],axis=0)\n",
    "\n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "   \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    " \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "Nothing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不做任何處理\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "accuracies=[]\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "   \n",
    "        \n",
    "    tempover = []\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata,output)\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1]\n",
    "    counter = Counter(test_y)\n",
    "    \"\"\"\n",
    "    for k,v in counter.items():\n",
    "\t    per = v / len(output) * 100\n",
    "\t    print(\"class\",k,\"數量：\",v,\"percentage\",'%.3f' %per,\"%\")\n",
    "\t\n",
    "    pyplot.bar(counter.keys(), counter.values())   \n",
    "    pyplot.show()  \n",
    "    \"\"\"\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "source": [
    "ADASYN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "import statistics\n",
    "accuracies=[]\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,data.shape[1] -1];\n",
    "    finaldata = data.iloc[:,:data.shape[1] -1]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    adsn = ADASYN()\n",
    "    new_X, new_y = adsn.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(new_X, new_y)\n",
    "    #clf = clf.fit(finaldata,output)\n",
    "    newDataCount = len(new_X) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "source": [
    "SMOTE 0.8220151828847481"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE \n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "accuracies=[]\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    smote = SMOTE()\n",
    "    new_X, new_y = smote.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "    print(len(new_X),\"dasd\")\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(new_X, new_y)\n",
    "    newDataCount = len(new_X) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    counter = Counter(test_y)\n",
    "    \n",
    "    counter = Counter(output)\n",
    " \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    \n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "source": [
    "Borderline-1  0.7979468599033817"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borderline-1\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "accuracies=[]\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    adsn =  BorderlineSMOTE(random_state=42,kind=\"borderline-1\")\n",
    "    new_X, new_y = adsn.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(new_X, new_y)\n",
    "    newDataCount = len(new_X) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    counter = Counter(test_y)\n",
    "    \n",
    "    counter = Counter(output)\n",
    "    \"\"\"\n",
    "    for k,v in counter.items():\n",
    "\t    per = v / len(output) * 100\n",
    "\t    print(\"class\",k,\"數量：\",v,\"percentage\",'%.3f' %per,\"%\")\n",
    "\t\n",
    "    pyplot.bar(counter.keys(), counter.values())\n",
    "    pyplot.show()\n",
    "    \"\"\"\n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    #accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "source": [
    "Borderline-2 0.8260006901311249"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borderline-2\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "accuracies=[]\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    Border2 =  BorderlineSMOTE(random_state=42,kind=\"borderline-2\")\n",
    "    new_X, new_y = Border2.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(new_X, new_y)\n",
    "    newDataCount = len(new_X) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    counter = Counter(test_y)\n",
    "    \n",
    "    counter = Counter(output)\n",
    "    \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    # 精確度\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE 取 7 成\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \"\"\"\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \"\"\"\n",
    "    alloverSMOTE = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.floor(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overSMOTE[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random ADsyn 取 3 成\n",
    "from imblearn.over_sampling import ADASYN\n",
    "alloverAdsyn = []\n",
    "overAdsyn = []\n",
    "randomAdsyn = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "   \n",
    "    alloverAdsyn = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    overAdsyn.append(alloverAdsyn)\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.ceil(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overAdsyn[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomAdsyn.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomAdsyn).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random borderline2\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "alloverBorder2 = []\n",
    "overBorder2 = []\n",
    "randomBorder2 = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    over = BorderlineSMOTE(random_state=42,kind=\"borderline-2\")\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "   \n",
    "    alloverBorder2 = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    overBorder2.append(alloverBorder2)\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        if(classCount[i][1] > 0):\n",
    "            count = math.ceil(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "            randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overBorder2[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomBorder2.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomBorder2).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRandomHalf = []\n",
    "temp = []\n",
    "# 合併 Adsyn SMOTE\n",
    "for i in range(len(randomAdsyn)):\n",
    "    #temp = randomAdsyn[i] + randomSMOTE[i]\n",
    "    randomAdsynpd = pd.DataFrame(randomAdsyn[i])\n",
    "    randomSMOTEpd = pd.DataFrame(randomSMOTE[i])\n",
    "    RandomHalf = pd.concat([randomAdsynpd,randomSMOTEpd],axis=0)\n",
    "    allRandomHalf.append(RandomHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRandomHalf = []\n",
    "temp = []\n",
    "# 合併 Adsyn Border2\n",
    "for i in range(len(randomAdsyn)):\n",
    "    #temp = randomAdsyn[i] + randomSMOTE[i]\n",
    "    randomAdsynpd = pd.DataFrame(randomAdsyn[i])\n",
    "    randomBorder2pd = pd.DataFrame(randomBorder2[i])\n",
    "    RandomHalf = pd.concat([randomAdsynpd,randomBorder2pd],axis=0)\n",
    "    allRandomHalf.append(RandomHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 合併三個\n",
    "allRandomHalf = []\n",
    "temp = []\n",
    "\n",
    "for i in range(len(randomAdsyn)):\n",
    "    #temp = randomAdsyn[i] + randomSMOTE[i]\n",
    "    randomAdsynpd = pd.DataFrame(randomAdsyn[i])\n",
    "    randomSMOTEpd = pd.DataFrame(randomSMOTE[i])\n",
    "    randomBorder2pd = pd.DataFrame(randomBorder2[i])\n",
    "    RandomHalf = pd.concat([randomAdsynpd,randomSMOTEpd],axis=0)\n",
    "    RandomHalf = pd.concat([RandomHalf,randomBorder2pd],axis=0)\n",
    "    allRandomHalf.append(RandomHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "    mergeRandom = pd.concat([data,allRandomHalf[index]],axis=0)\n",
    "    #print(mergeRandom)\n",
    "    \n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(output)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "    mergeRandom = pd.concat([data,allRandomHalf[index]],axis=0)\n",
    "    #print(mergeRandom)\n",
    "    \n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "    test_y = le.fit_transform(test_y)\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allRandomHalf[0])"
   ]
  },
  {
   "source": [
    "SMOTE 0.7  ADASYN 0.3  準確度 0.7989130\n",
    "SMOTE 0.3  ADASYN 0.7  準確度 0.7989130\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "群中心 for 個別"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  \n",
    "kmeans = KMeans(n_clusters=1)\n",
    "X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[1]\n",
    "output = data.iloc[:,data.shape[1]-1];\n",
    "output[:]"
   ]
  },
  {
   "source": [
    "SMOTE 群中心"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 群中心 index\n",
    "from imblearn.over_sampling import  SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "centerSMOTE = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverSMOTE = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overSMOTE[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerSMOTE.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerSMOTE[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSMOTEmin 只是 index ，value 是取出值\n",
    "centerSMOTEvalue =[]\n",
    "for i in range(len(centerSMOTE)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerSMOTE[i])):\n",
    "        indexSMOTE = centerSMOTE[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(overSMOTE[i].iloc[indexSMOTE]))\n",
    "    centerSMOTEvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(centerSMOTEvalue[0])"
   ]
  },
  {
   "source": [
    "ADASYN 群中心"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN 群中心 index\n",
    "from imblearn.over_sampling import  ADASYN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverADASYN = []\n",
    "overADASYN = []\n",
    "randomADASYN = []\n",
    "centerADASYN = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverADASYN = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overADASYN.append(alloverADASYN)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.7); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overADASYN[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerADASYN.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerADASYN[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerADASYN 只是 index ，value 是取出值\n",
    "centerADASYNvalue =[]\n",
    "for i in range(len(centerADASYN)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerADASYN[i])):\n",
    "        indexSMOTE = centerADASYN[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(overADASYN[i].iloc[indexSMOTE]))\n",
    "    centerADASYNvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center ADASYN + SMOTE 各 \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerADASYN)):\n",
    "    temp = centerADASYNvalue[i] + centerSMOTEvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterHalf[index]],axis=0)\n",
    "\n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    #test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "計算原資料小類10筆的群中心, 分別選出離此小類群中心各一定比例的資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 小類 以及算出 群中心\n",
    "from sklearn.cluster import KMeans \n",
    "mincenter = []\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    mintype = 0;\n",
    "    minlist = [] # collect 小類數據集\n",
    "    for i in range(len(classCount)):\n",
    "        if(classCount[i][1] != '0'):\n",
    "            mintype = classCount[i][0];\n",
    "    for j in range(data.shape[0]):\n",
    "        if(output.iloc[j] == mintype):\n",
    "            minlist.append(data.iloc[j,:])\n",
    "    minpd = pd.DataFrame(minlist)\n",
    "    #minpd = minpd.reset_index()\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    kmeans.fit(minpd.iloc[:,:minpd.shape[1]-1])\n",
    "    #y_kmeans = kmeans.predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    mincenter.extend(centers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓出距離小類群中心最近的一半 data \n",
    "# SMOTE \n",
    "from imblearn.over_sampling import  SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "centerSMOTEmin = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverSMOTE = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            #kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overSMOTE[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            #kmeans.fit(X)\n",
    "            #y_kmeans = kmeans.predict(X)\n",
    "            #centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            #centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "        \n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((mincenter[ii][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerSMOTEmin.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerSMOTEmin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSMOTEmin 只是 index ，value 是取出值\n",
    "centerSMOTEvalue =[]\n",
    "for i in range(len(centerSMOTEmin)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerSMOTEmin[i])):\n",
    "        indexSMOTE = centerSMOTEmin[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(overSMOTE[i].iloc[indexSMOTE]))\n",
    "    centerSMOTEvalue.append(alltemp)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓出距離小類群中心最近的一半 data \n",
    "# ADASYN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverADASYN = []\n",
    "overADASYN = []\n",
    "randomADASYN = []\n",
    "centerADASYNmin = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverADASYN = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後全部的數據\n",
    "    \n",
    "    overADASYN.append(alloverADASYN)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            #kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overADASYN[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            #kmeans.fit(X)\n",
    "            #y_kmeans = kmeans.predict(X)\n",
    "            #centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            #centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "        \n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((mincenter[ii][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerADASYNmin.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerADASYNmin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerADASYNvalue =[]\n",
    "for i in range(len(centerADASYNmin)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerADASYNmin[i])):\n",
    "        indexSMOTE = centerADASYNmin[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(overADASYN[i].iloc[indexSMOTE]))\n",
    "    centerADASYNvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center ADASYN + SMOTE 各 \n",
    "allCenterminHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerADASYNmin)):\n",
    "    temp = centerADASYNvalue[i] + centerSMOTEvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterminHalf.append(temp)\n",
    "\n",
    "len(allCenterminHalf[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterminHalf[index]],axis=0)\n",
    "\n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    #test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "計算原資料小類10筆的群中心, 然後將 OS1 和 OS2 產生的180筆選最接近此群中心的 90 筆"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE 取 7 成\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    #data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    #finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    #print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \"\"\"\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \"\"\"\n",
    "    alloverSMOTE = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.floor(int(classCount[i][1])); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overSMOTE[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overSMOTE[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random ADsyn 取 3 成\n",
    "from imblearn.over_sampling import ADASYN\n",
    "alloverAdsyn = []\n",
    "overAdsyn = []\n",
    "randomAdsyn = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    \n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    \n",
    "    print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "   \n",
    "    alloverAdsyn = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後的數據\n",
    "    #print(len(alloverADASYN),\"length\")\n",
    "    overAdsyn.append(alloverAdsyn)\n",
    "\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.ceil(int(classCount[i][1])); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overAdsyn[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomAdsyn.append(randomtemp)\n",
    "    #print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomAdsyn).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(randomSMOTE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRandomHalf = []\n",
    "temp = []\n",
    "# 合併 Adsyn SMOTE\n",
    "for i in range(len(randomAdsyn)):\n",
    "    #temp = randomAdsyn[i] + randomSMOTE[i]\n",
    "    randomAdsynpd = pd.DataFrame(randomAdsyn[i])\n",
    "    randomSMOTEpd = pd.DataFrame(randomSMOTE[i])\n",
    "    RandomHalf = pd.concat([randomAdsynpd,randomSMOTEpd],axis=0)\n",
    "    allRandomHalf.append(RandomHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allRandomHalf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓出距離小類群中心最近的一半 data \n",
    "# ADASYN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "alloverADASYN = []\n",
    "overADASYN = []\n",
    "randomADASYN = []\n",
    "centerADASYNmin = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    over = ADASYN()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    alloverADASYN = pd.concat([X_smote,y_smote],axis=1) # SMOTE 完後全部的數據\n",
    "    \n",
    "    overADASYN.append(alloverADASYN)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.5); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            #kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overADASYN[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            #kmeans.fit(X)\n",
    "            #y_kmeans = kmeans.predict(X)\n",
    "            #centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            #centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "        \n",
    "            for i in range(X.shape[0]): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]):#9 行\n",
    "                    temp = pow((mincenter[ii][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerADASYNmin.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerADASYNmin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 小類 以及算出 群中心\n",
    "from sklearn.cluster import KMeans \n",
    "mincenter = []\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    mintype = 0;\n",
    "    minlist = [] # collect 小類數據集\n",
    "    for i in range(len(classCount)):\n",
    "        if(classCount[i][1] != '0'):\n",
    "            mintype = classCount[i][0];\n",
    "    for j in range(data.shape[0]):\n",
    "        if(output.iloc[j] == mintype):\n",
    "            minlist.append(data.iloc[j,:])\n",
    "    minpd = pd.DataFrame(minlist)\n",
    "    #minpd = minpd.reset_index()\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    kmeans.fit(minpd.iloc[:,:minpd.shape[1]-1])\n",
    "    #y_kmeans = kmeans.predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    mincenter.extend(centers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合所有合成 data 後 取出鄰近小類群中心的 data\n",
    "centerallmin = []\n",
    "\n",
    "for ii,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "    \n",
    "    X = allRandomHalf[ii].iloc[:,:allRandomHalf[ii].shape[1]-1]\n",
    "    #print(X)\n",
    "    for i in range(len(classCount)):\n",
    "            countfor = math.floor(int(classCount[i][1])); # 要產生多少數據  無條件捨去\n",
    "            #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "            print(\"classcount\",countfor)\n",
    "\n",
    "            if(countfor>0):\n",
    "                distance = []\n",
    "                X = X.astype('float64')\n",
    "                #centers = centers.astype('float64')\n",
    "                tempindata = {}\n",
    "                distancesortemp = []\n",
    "                for ro in range(X.shape[0]):\n",
    "                    for j in range(X.shape[1]):#9 行\n",
    "                        temp = pow((mincenter[ii][j]-X.iloc[ro][j]),2)  \n",
    "                       \n",
    "                        tempindata[ro] = temp\n",
    "            \n",
    "                distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "                #print(distancesortemp)\n",
    "     \n",
    "                centerallmin.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerallmin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerallvalue =[]\n",
    "for i in range(len(centerallmin)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerallmin[i])):\n",
    "        indexSMOTE = centerallmin[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(allRandomHalf[i].iloc[indexSMOTE]))\n",
    "    \n",
    "    centerallvalue.append(alltemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    centerallvaluepd = pd.DataFrame(centerallvalue[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,centerallvaluepd],axis=0)\n",
    "\n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    #test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "分別產生各 data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 利用 sampling strategy 產出不同資料數\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "sampleSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    \n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    #finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    tempover = []\n",
    "    smote = SMOTE(sampling_strategy=0.5)\n",
    "    originlen = len(data) #要去掉原先的 data\n",
    "    new_X, new_y = smote.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "\n",
    "    temp = pd.concat([new_X,new_y],axis=1)\n",
    "    temp = temp.iloc[originlen:,:]\n",
    "    sampleSMOTE.append(temp)\n",
    "   \n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ADASYN 利用 sampling strategy 產出\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "sampleADASYN = []\n",
    "for ii,i in enumerate(train):\n",
    "\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    #finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    originlen = len(data)\n",
    "    tempover = []\n",
    "    smote = ADASYN(sampling_strategy=0.5)\n",
    "    new_X, new_y = smote.fit_sample(finaldata,output)  # your imbalanced dataset is in X,y\n",
    "    temp = pd.concat([new_X,new_y],axis=1)\n",
    "    temp = temp.iloc[originlen:,:]\n",
    "    sampleADASYN.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併兩者 此法可能會少數據\n",
    "allSample = []\n",
    "temp = []\n",
    "# 合併 Adsyn SMOTE\n",
    "for i in range(len(sampleSMOTE)):\n",
    "    #temp = randomAdsyn[i] + randomSMOTE[i]\n",
    "    sampleADASYNpd = pd.DataFrame(sampleADASYN[i])\n",
    "    sampleSMOTEpd = pd.DataFrame(sampleSMOTE[i])\n",
    "    sampletemp = pd.concat([sampleADASYNpd,sampleSMOTEpd],axis=0)\n",
    "    allSample.append(sampletemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = allSample[0].iloc[:,allSample[0].shape[1]-1]\n",
    "outc = Counter(out)\n",
    "print(outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    # 合併原先 data 跟 新生成的值\n",
    "    mergeRandom = pd.concat([data,allSample[ii]],axis=0)\n",
    "\n",
    "    output = mergeRandom.iloc[:,l];\n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    c = Counter(output)\n",
    "    print(c)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output) # 使用 decision tree 預測\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    \n",
    "    le = preprocessing.LabelEncoder() # 使 string 轉成\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}