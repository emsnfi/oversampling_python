{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os;\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/emily/Desktop/Research/oversampling_python\"\n",
    "dirs = os.listdir(path+\"/data/pima-5-fold\")\n",
    "os.chdir(path+\"/data/pima-5-fold\")\n",
    "# 到時候 path 這邊也要 loop 不同資料集的資料夾\n",
    "# 输出所有文件和文件夹\n",
    "train = []\n",
    "test = []\n",
    "allfile = []\n",
    "for file in dirs:\n",
    "    if(\"xlsx\" in file):\n",
    "        allfile.append(file)\n",
    "        if(\"tra\" in file ):\n",
    "            train.append(file)\n",
    "        elif(\"tst\" in file):\n",
    "            test.append(file)\n",
    "allfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "import statistics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "os.chdir(\"/Users/emily/Desktop/Research/oversampling_python/data/pima-5-fold\")\n",
    "accuracies=[]\n",
    "oversampleRCSMOTE = [] # 全部 5 個 train data 所訓練出來的 oversample data\n",
    "for i in train:\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    \n",
    "    tempover = []\n",
    "    for i in range(len(classCount)):\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            tempover.extend(over)\n",
    "            \n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "    X = alldata.T.iloc[:,:alldata.shape[0]-1]\n",
    "    y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X, y) # 訓練 model\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    "    \"\"\" 畫樹\n",
    "    dot_data = tree.export_graphviz(clf)\n",
    "    fig = pyplot.figure(figsize=(25,20))\n",
    "    _ = tree.plot_tree(clf)\n",
    "    tree.plot_tree(clf) \n",
    "    \"\"\"\n",
    "    test_file = pd.read_excel(test[i],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(test_data.shape[1]):\n",
    "        test_data.iloc[:,i] = le.fit_transform(test_data.iloc[:,i]) \n",
    "    \"\"\"\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "overSMOTE = []\n",
    "alloverSMOTE = []\n",
    "for i in train:\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "    tempover = []\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    \n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    \n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    \n",
    "    #tempover.extend([X_smote,y_smote])\n",
    "\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    print(len(overSMOTE),\"len\")\n",
    "    alloverSMOTE = [] \n",
    "print(len(overSMOTE[0]))\n",
    "np.array(overSMOTE).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranodom half SMOTE \n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        count = math.floor(int(classCount[i][1])/2); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "\n",
    "    \n",
    "    for index in randomIndex:\n",
    "       \n",
    "        randomtemp.append(overSMOTE[ii][index])\n",
    "        \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 random RCSMOTE\n",
    "\n",
    "randomRCSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    randomIndex = []\n",
    "    tempover = []\n",
    "    for j in range(len(classCount)):\n",
    "        if(int(classCount[j][1]) > 0):\n",
    "            over = Populate(int(classCount[j][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[j][0])\n",
    "            tempover.append(over)\n",
    "            #length = alldata.shape[1]\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    " \n",
    "    \n",
    "    for a in range(len(classCount)): # 產生 random index\n",
    "        count = math.ceil(int(classCount[a][1])/2); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(0,len(oversampleRCSMOTE[ii])-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(\"randomindex\",len(randomIndex))\n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(oversampleRCSMOTE[ii][index])\n",
    "    randomRCSMOTE.append(randomtemp)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE + RCSMOTE 各一半的結果 \n",
    "allRandomHalf = []\n",
    "temp = []\n",
    "for i in range(len(randomRCSMOTE)):\n",
    "    temp = randomRCSMOTE[i] + randomSMOTE[i]\n",
    "    allRandomHalf.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    classCount = classprocess(output)\n",
    "    data = data.T\n",
    "    \n",
    "    \n",
    "    for i in range(len(allRandomHalf[index])): # 185\n",
    "        #for j in range(len(allRandomHalf[index][0])): #9\n",
    "            datalength = data.shape[1]\n",
    "            data[datalength+i+1] = allRandomHalf[index][j]\n",
    "    #print(data)  \n",
    "        #print(datalength)\n",
    "        #print(allRandomHalf[index][i],\"sl;d\")\n",
    "    mergeRandom = data.T\n",
    "    l = mergeRandom.shape[1]-1\n",
    "    output = np.array(mergeRandom.iloc[:,l]);\n",
    "    finaldata = np.array(mergeRandom.iloc[:,:l])\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\"\"\"\n",
    "    for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "\"\"\"\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "不同參數的合併 random"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "RCSMOTE : SMOTE = 7:3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 random RCSMOTE 取 7 成\n",
    "\n",
    "randomRCSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    randomIndex = []\n",
    "    tempover = []\n",
    "    for j in range(len(classCount)):\n",
    "        if(int(classCount[j][1]) > 0):\n",
    "            over = Populate(int(classCount[j][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[j][0])\n",
    "            tempover.append(over)\n",
    "            #length = alldata.shape[1]\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    " \n",
    "    \n",
    "    for a in range(len(classCount)): # 產生 random index\n",
    "        count = math.ceil(int(classCount[a][1])*0.7); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(0,len(oversampleRCSMOTE[ii])-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(\"randomindex\",len(randomIndex))\n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(oversampleRCSMOTE[ii][index])\n",
    "    randomRCSMOTE.append(randomtemp)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE 取 3 成\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        count = math.floor(int(classCount[i][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "\n",
    "    \n",
    "    for index in randomIndex:\n",
    "       \n",
    "        randomtemp.append(overSMOTE[ii][index])\n",
    "        \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE : RCSMOTE = 3:7 的結果 \n",
    "allRandomHalf = []\n",
    "temp = []\n",
    "for i in range(len(randomRCSMOTE)):\n",
    "    temp = randomRCSMOTE[i] + randomSMOTE[i]\n",
    "    allRandomHalf.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    classCount = classprocess(output)\n",
    "    data = data.T\n",
    "    \n",
    "    \n",
    "    for i in range(len(allRandomHalf[index])): # 185\n",
    "        #for j in range(len(allRandomHalf[index][0])): #9\n",
    "            datalength = data.shape[1]\n",
    "            data[datalength+i+1] = allRandomHalf[index][j]\n",
    "    #print(data)  \n",
    "        #print(datalength)\n",
    "        #print(allRandomHalf[index][i],\"sl;d\")\n",
    "    mergeRandom = data.T\n",
    "    l = mergeRandom.shape[1]-1\n",
    "    output = np.array(mergeRandom.iloc[:,l]);\n",
    "    finaldata = np.array(mergeRandom.iloc[:,:l])\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\"\"\"\n",
    "    for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "\"\"\"\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "SMOTE : RCSMOTE = 7 : 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 random RCSMOTE 取 3 成\n",
    "\n",
    "randomRCSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    randomIndex = []\n",
    "    tempover = []\n",
    "    for j in range(len(classCount)):\n",
    "        if(int(classCount[j][1]) > 0):\n",
    "            over = Populate(int(classCount[j][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[j][0])\n",
    "            tempover.append(over)\n",
    "            #length = alldata.shape[1]\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    " \n",
    "    \n",
    "    for a in range(len(classCount)): # 產生 random index\n",
    "        count = math.ceil(int(classCount[a][1])*0.3); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(0,len(oversampleRCSMOTE[ii])-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(\"randomindex\",len(randomIndex))\n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(oversampleRCSMOTE[ii][index])\n",
    "    randomRCSMOTE.append(randomtemp)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE 取 7 成\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        count = math.floor(int(classCount[i][1])*0.7); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "\n",
    "    \n",
    "    for index in randomIndex:\n",
    "       \n",
    "        randomtemp.append(overSMOTE[ii][index])\n",
    "        \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE : RCSMOTE = 3:7 的結果 \n",
    "allRandomHalf = []\n",
    "temp = []\n",
    "for i in range(len(randomRCSMOTE)):\n",
    "    temp = randomRCSMOTE[i] + randomSMOTE[i]\n",
    "    allRandomHalf.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    classCount = classprocess(output)\n",
    "    data = data.T\n",
    "    \n",
    "    \n",
    "    for i in range(len(allRandomHalf[index])): # 185\n",
    "        #for j in range(len(allRandomHalf[index][0])): #9\n",
    "            datalength = data.shape[1]\n",
    "            data[datalength+i+1] = allRandomHalf[index][j]\n",
    "    #print(data)  \n",
    "        #print(datalength)\n",
    "        #print(allRandomHalf[index][i],\"sl;d\")\n",
    "    mergeRandom = data.T\n",
    "    l = mergeRandom.shape[1]-1\n",
    "    output = np.array(mergeRandom.iloc[:,l]);\n",
    "    finaldata = np.array(mergeRandom.iloc[:,:l])\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\"\"\"\n",
    "    for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "\"\"\"\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "群中心正式\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "191\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "191\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "215\n",
      "400\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "161\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "161\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "215\n",
      "400\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "172\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "172\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "214\n",
      "400\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "181\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "181\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "214\n",
      "400\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "183\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "183\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "214\n",
      "400\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(72, 0.13520767671410652),\n",
       " (74, 0.1881293571827031),\n",
       " (85, 0.7441252119817483),\n",
       " (105, 5.011220890354801),\n",
       " (154, 7.8829047482268715),\n",
       " (60, 9.189292188689743),\n",
       " (151, 9.259300824418853),\n",
       " (181, 10.362224430827212),\n",
       " (161, 12.640067352120113),\n",
       " (158, 15.912326580083768),\n",
       " (23, 16.259322015210323),\n",
       " (94, 17.547723832079022),\n",
       " (34, 17.617737324917325),\n",
       " (3, 17.702194372323184),\n",
       " (104, 18.590976757782094),\n",
       " (12, 19.77488743723588),\n",
       " (31, 21.641672626425915),\n",
       " (170, 21.91108217617063),\n",
       " (180, 24.17743196577719),\n",
       " (26, 26.275571420739016),\n",
       " (96, 26.608508787397277),\n",
       " (176, 31.489621397048353),\n",
       " (115, 34.90210212443601),\n",
       " (125, 36.16077856395299),\n",
       " (122, 36.4446906582301),\n",
       " (79, 41.77443076410595),\n",
       " (112, 44.38227711329824),\n",
       " (152, 44.80040198248739),\n",
       " (86, 47.04622736690066),\n",
       " (102, 52.05443523568941),\n",
       " (51, 54.652952824113235),\n",
       " (70, 56.584505048618276),\n",
       " (136, 60.270449643852025),\n",
       " (166, 60.9512278598697),\n",
       " (163, 65.78700304330995),\n",
       " (100, 66.76230959937779),\n",
       " (150, 67.52545697673258),\n",
       " (40, 73.35652837863293),\n",
       " (35, 73.44934764418727),\n",
       " (17, 73.78590387788992),\n",
       " (146, 76.22283219141471),\n",
       " (98, 90.60065375479994),\n",
       " (47, 94.14969714074712),\n",
       " (97, 98.5065733277094),\n",
       " (119, 102.63241121752937),\n",
       " (68, 102.9079777674818),\n",
       " (121, 104.66513324899152),\n",
       " (118, 110.05728584458303),\n",
       " (165, 115.13288675840957),\n",
       " (82, 118.73771898500509),\n",
       " (13, 120.99692945982628),\n",
       " (0, 121.60295777576394),\n",
       " (28, 124.7716962278907),\n",
       " (63, 128.05486933368553),\n",
       " (177, 138.5860621215068),\n",
       " (110, 143.68528136921478)]"
      ]
     },
     "metadata": {},
     "execution_count": 877
    }
   ],
   "source": [
    "# RCSMOTE 群中心 取 一半 50%\n",
    "from sklearn.cluster import KMeans  \n",
    "randomRCSMOTE = []\n",
    "centerRCSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    randomIndex = []\n",
    "    tempover = []\n",
    "    for j in range(len(classCount)):\n",
    "        if(int(classCount[j][1]) > 0):\n",
    "            over = Populate(int(classCount[j][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[j][0])\n",
    "            tempover.append(over)\n",
    "            #length = alldata.shape[1]\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    "    countfor = math.ceil(len(oversampleRCSMOTE[ii])*0.3)\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    dtemp = pd.DataFrame(oversampleRCSMOTE[ii])\n",
    "    X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    distance = []\n",
    "    X = X.astype('float64')\n",
    "    centers = centers.astype('float64')\n",
    "    tempindata = {}\n",
    "    for i in range(X.shape[0]-1): #129 列\n",
    "        distance = []\n",
    "        temp = 0;\n",
    "        for j in range(X.shape[1]-1):#9 行\n",
    "            temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "            tempindata[i] = temp\n",
    "    \n",
    "    distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "        #print(sorted(k.items(), key=lambda x:x[1]))\n",
    "        #print(tempindata)\n",
    "        #distance.append(tempindata)\n",
    "        #print(np.array(distance).shape)\n",
    "    #distancearr = np.array([distancesortemp.items,)\n",
    "    centerRCSMOTE.append(distancesortemp[:countfor])\n",
    "    #print(distancesortemp[:countfor])\n",
    "    \n",
    "    #distancesort.append(distancesortemp)\n",
    "centerRCSMOTE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "215\n",
      "400\n",
      "215\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# SMOTE 群中心 一半\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "centerSMOTE = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.7); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overSMOTE[ii])\n",
    "            X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            for i in range(X.shape[0]-1): #129 列\n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "    \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "        #print(sorted(k.items(), key=lambda x:x[1]))\n",
    "        #print(tempindata)\n",
    "        #distance.append(tempindata)\n",
    "        #print(np.array(distance).shape)\n",
    "    #distancearr = np.array([distancesortemp.items,)\n",
    "            centerSMOTE.append(distancesortemp[:countfor])\n",
    "    #print(distancesortemp[:countfor])\n",
    "    \n",
    "    #distancesort.append(distancesortemp)\n",
    "print(len(centerSMOTE[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSOMTE centerRCSMOTE\n",
    "# 原先 data overSMOTE  oversampleRCSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "metadata": {},
     "execution_count": 879
    }
   ],
   "source": [
    "# centerSMOTEvalue  紀錄實際在群中心的 SMOTE 一半\n",
    "centerSMOTEvalue =[]\n",
    "for i in range(len(centerSMOTE)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerSMOTE[i])):\n",
    "        indexSMOTE = centerSMOTE[i][j][0]\n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(list(overSMOTE[i][indexSMOTE]))\n",
    "    centerSMOTEvalue.append(alltemp)\n",
    "  \n",
    "len(centerSMOTEvalue[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerRCSMOTEvalue  紀錄實際在群中心的 RCSMOTE 一半\n",
    "\n",
    "centerRCSMOTEvalue =[]\n",
    "for i in range(len(centerRCSMOTE)):\n",
    "    alltemp = []\n",
    "    for j in range(len(centerRCSMOTE[i])):\n",
    "        #indexSMOTE = centerSMOTE[i][j][0]\n",
    "        indexRCSMOTE = centerRCSMOTE[i][j][0]\n",
    "       \n",
    "        #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "        alltemp.append(oversampleRCSMOTE[i][indexRCSMOTE])\n",
    "    centerRCSMOTEvalue.append(alltemp)\n",
    "        #print(type(tempSMOTE))\n",
    "        #temp = tempSMOTE + tempRCSMOTE\n",
    "        #alltemp.append(temp)\n",
    "    #allCenterHalf.append(alltemp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 881
    }
   ],
   "source": [
    "# 合併 center RCSMOTE + SMOTE 各 50 \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerRCSMOTE)):\n",
    "    temp = centerRCSMOTEvalue[i] + centerSMOTEvalue[i]\n",
    "    allCenterHalf.append(temp)\n",
    "len(allCenterHalf[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "215\n",
      "400\n",
      "215\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "0.8929971988795519\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    for i in range(len(classCount)):\\n    #print(int(classCount[i][1]))\\n        if(int(classCount[i][1]) > 0):\\n            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\\n            length = alldata.shape[1]\\n            for j in range(len(over)):\\n                alldata[length+j] = over[j]\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 882
    }
   ],
   "source": [
    "# 測試\n",
    "# 跟原始資料合併\n",
    "mergeCenter = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    classCount = classprocess(output)\n",
    "    data = data.T\n",
    "    \n",
    "    \n",
    "    for i in range(len(allCenterHalf[index])): # 185\n",
    "        #for j in range(len(allRandomHalf[index][0])): #9\n",
    "            datalength = data.shape[1]\n",
    "            data[datalength+i+1] = allCenterHalf[index][j]\n",
    "    #print(data)  \n",
    "        #print(datalength)\n",
    "        #print(allRandomHalf[index][i],\"sl;d\")\n",
    "    mergeCenter = data.T\n",
    "    l = mergeCenter.shape[1]-1\n",
    "    output = np.array(mergeCenter.iloc[:,l]);\n",
    "    finaldata = np.array(mergeCenter.iloc[:,:l])\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\"\"\"\n",
    "    for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "\"\"\"\n",
    "#len(mergeRandom[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找群中心\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=1)\n",
    "X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "distance = []\n",
    "X = X.astype('float64')\n",
    "centers = centers.astype('float64')\n",
    "for i in range(X.shape[0]-1): #129 列\n",
    "    temp = 0;\n",
    "    for j in range(X.shape[1]-1):#9 行\n",
    "        temp = pow((centers[0][j]-X.iloc[i][j]),2)\n",
    "        #print(centers[0][j])\n",
    "        #print(centers[0][j]-X.iloc[i][j])\n",
    "    distance.append(temp)\n",
    "distancesort = sorted(distance)\n",
    "distancesort"
   ]
  },
  {
   "source": [
    "群中心測試"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns; \n",
    "#sns.set()\n",
    "import numpy as np\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtemp = pd.DataFrame(randomSMOTE[0])\n",
    "X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X.iloc[0,:])\n",
    "w = np.array(X.iloc[0,:])\n",
    "type(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找群中心\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=1)\n",
    "X = dtemp.iloc[:,:dtemp.shape[1]-1]\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "distance = []\n",
    "X = X.astype('float64')\n",
    "centers = centers.astype('float64')\n",
    "for i in range(X.shape[0]-1): #129 列\n",
    "    temp = 0;\n",
    "    for j in range(X.shape[1]-1):#9 行\n",
    "        temp = pow((centers[0][j]-X.iloc[i][j]),2)\n",
    "        #print(centers[0][j])\n",
    "        #print(centers[0][j]-X.iloc[i][j])\n",
    "    distance.append(temp)\n",
    "distancesort = sorted(distance)\n",
    "distancesort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " neighbors=NearestNeighbors(n_neighbors=10).fit(X)\n",
    " temp = neighbors.kneighbors(X,return_distance=False)\n",
    " print(centers)\n",
    "index = \"no\"\n",
    "\"\"\"\n",
    "for i in range(len(centers[0])):\n",
    "    for j in X\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "for i in range(X.shape[0]):\n",
    "    w = np.array(X.iloc[i,:])\n",
    "    print(w)\n",
    "    if(centers == w.any()):\n",
    "        index = i;\n",
    "        print(\"yes\",index)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, datasets, metrics\n",
    "# 讀入鳶尾花資料\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "\n",
    "# KMeans 演算法\n",
    "kmeans_fit = cluster.KMeans(n_clusters = 4).fit(iris_X)\n",
    "centers = kmeans_fit.cluster_centers_\n",
    "# 印出分群結果\n",
    "cluster_labels = kmeans_fit.labels_\n",
    "print(\"分群結果：\")\n",
    "print(cluster_labels)\n",
    "print(\"---\")\n",
    "\n",
    "# 印出品種看看\n",
    "iris_y = iris.target\n",
    "print(\"真實品種：\")\n",
    "print(iris_y)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas._testing import assert_frame_equal\n",
    "df3 = pd.DataFrame(overSMOTE[0],columns=data.columns,dtype=object)\n",
    "df3.index += 1\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas._testing import assert_frame_equal\n",
    "df1 = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
    "df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})\n",
    "a = assert_frame_equal(df1, df1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling SMOTE + RCSMOTE  Random 選擇 一半一半\n",
    "# 沒有作用\n",
    "import random \n",
    "import math\n",
    "classCount = classprocess(output)\n",
    "randomSmote = []\n",
    "tempRCSMOTE = []\n",
    "tempSMOTE = []\n",
    "BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "Range = range_value(3,finaldata,output,data)\n",
    "P_max = range_value(3,finaldata,output,data)[0]\n",
    "Range = range_value(3,finaldata,output,data)[2]\n",
    "for i in range(len(classCount)):\n",
    "    count = math.ceil(int(classCount[i][1])/2); # 要產生多少數據  無條件捨去\n",
    "    randomSmote.append([random.randint(0, len(output)-1) for _ in range(count)])\n",
    "print(randomSmote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[1] # 行數    data.shape[0] 列數\n",
    "os.chdir(\"/Users/emily/Desktop/Research/oversampling_python/data/\");\n",
    "#print(os.getcwd());\n",
    "\n",
    "data = pd.read_excel(\"pima.xlsx\")\n",
    "data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "\n",
    "data = data.iloc[:,1:]\n",
    "data.shape[1]\n",
    "l = data.shape[1]-1\n",
    "\n",
    "output = np.array(data.iloc[:,l]);\n",
    "finaldata = np.array(data.iloc[:,:l])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(output)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedata = data\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in range(tedata.shape[1]):\n",
    "    tedata.iloc[:,i] = le.fit_transform(tedata.iloc[:,i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分佈\n",
    "counter = Counter(output)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(output) * 100\n",
    "\tprint(\"class\",k,\"數量：\",v,\"percentage\",'%.3f' %per,\"%\")\n",
    "\t#print('Class=%s, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()\n",
    "ir = 500/268;\n",
    "print(\"IR \",ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出 knn\n",
    "def knn(n_nu,sample):\n",
    "     # n_nu 自己設定多少鄰近個點\n",
    "     # input sample 沒有包含 class label\n",
    "        nnarray = []\n",
    "        neighbors=NearestNeighbors(n_neighbors=n_nu+1).fit(sample)\n",
    "        for i in range(len(sample)):\n",
    "            temp = neighbors.kneighbors(sample[i].reshape(1,-1),return_distance=False)[0]\n",
    "            temp = np.delete(temp,0)\n",
    "            # 有 array 存放 各個點的鄰近點 \n",
    "            nnarray.append(temp)\n",
    "        return nnarray # 回傳相近的點 分別是在第幾個 \n",
    "karray = knn(3,finaldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出大類\n",
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    return  maj\n",
    "\n",
    "find_maj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷該點為什麼類型的點 \n",
    "# 此 index 是 0 開始 所以對照到 excel 的要\n",
    "# inminority 是否是要求在小類中的 如果是則放 true\n",
    "def check_point_type(n_nu,sample,sample_class,data,inminority):\n",
    "    # 使用 find_maj(find_maj(sample_class)\n",
    "    # 使用 knn 找到鄰近的幾個點 \n",
    "    point_type = [] # 放個點是屬於什麼類型的 border safe noisy\n",
    "    n = 0\n",
    "    b = 0\n",
    "    s = 0\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    point = knn(n_nu,sample) # 回傳所有點鄰近 n_nu 個點\n",
    "    #maj = \"\".join(maj) # maj 原本是 list 轉成 str\n",
    "    for index,i in enumerate(point):\n",
    "        maj_nu =0;\n",
    "        if(data.iloc[index,data.shape[1]-1] == maj and inminority == True): # 如果該點本身是大類則不計算\n",
    "            continue; # 如果使用此 則會跟論文的 data 一樣\n",
    "        for j in point[index]: # 每個點鄰近個點的 loop 例如第一個點是 [1,2,3,4,5] 則 loop 裡面的數值\n",
    "            if(data.iloc[j,data.shape[1]-1] == maj ): \n",
    "                maj_nu = maj_nu + 1 # 計算鄰近點為大類的有多少個\n",
    "        if(maj_nu == n_nu ):\n",
    "            point_type.append(\"noisy\");\n",
    "            n = n+1\n",
    "        elif(n_nu/2 < maj_nu and maj_nu < n_nu ):\n",
    "            point_type.append(\"border\");\n",
    "            b = b+1\n",
    "        else:\n",
    "            point_type.append(\"safe\");\n",
    "            s = s+1\n",
    "    print(\"noisy\",n,\"\\nborder\",b,\"\\nsafe\",s,\"\\nall\",n+b+s)\n",
    "    return point_type;\n",
    "point_type = check_point_type(3,finaldata,output,data,True) # 論文使用 knn 為 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集變為小類的 border 跟 safe 以及 大類 去除 小類且為 noisy 的\n",
    "\"\"\"\n",
    "find the maximum value that existed for that attribute among\n",
    "the minority samples of the 𝐵𝑆 set, and also, find the minimum value that existed for\n",
    "the given attribute among the majority samples set\n",
    "\"\"\"\n",
    "# 先是小類去掉 noisy 再來是 大類 \n",
    "def split_BS_Majdata(point_type,sample,sample_class):\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    #maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(sample[i])\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(sample[i])\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "  \n",
    "    return return_sample\n",
    "point_type = check_point_type(3,finaldata,output,data,False)\n",
    "split_BS_Majdata(point_type,finaldata,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_split_BS_Majdata(point_type,sample,sample_class): # maj 以及 BS set 在原本 data 的 index\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    #maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(i)\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(i)\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "    \n",
    "    return return_sample\n",
    "point_type = check_point_type(3,finaldata,output,data,False)\n",
    "index_split_BS_Majdata(point_type,finaldata,output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Counter(output)\n",
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap\n",
    "\n",
    "classprocess(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The obtained ranges for\n",
    "all features (attributes) are used to control the location of the new synthetic samples in\n",
    "data space.\n",
    "\n",
    "The set of these ranges is denoted by 𝑅𝑎𝑛𝑔𝑒 =\n",
    "(𝑟𝑎𝑛𝑔𝑒1 𝑟𝑎𝑛𝑔𝑒2 𝑟𝑎𝑛𝑔𝑒3 … 𝑟𝑎𝑛𝑔𝑒𝑛𝑎𝑡𝑡𝑟) array where 𝑛𝑎𝑡𝑡𝑟 is the number of\n",
    "attributes in the dataset\n",
    "\n",
    "如何產生 Range array\n",
    "1. we find the maximum value that existed for that attribute among\n",
    "the minority samples of the 𝐵𝑆 set  𝑃 _𝑚𝑎𝑥 = (𝑝𝑚𝑎𝑥1 𝑝𝑚𝑎𝑥2\n",
    "…𝑝𝑚𝑎𝑥𝑛𝑎𝑡𝑡𝑟) array\n",
    "\n",
    "2. find the minimum value that existed for\n",
    "the given attribute among the majority samples set  𝑁 _𝑚𝑖𝑛 = (𝑛𝑚𝑖𝑛1 𝑛𝑚𝑖𝑛2 …𝑛𝑚𝑖𝑛𝑛𝑎𝑡𝑡𝑟)array\n",
    "\n",
    "3. Then, the desired 𝑅𝑎𝑛𝑔𝑒 vector is obtained as the average of 𝑁_𝑚𝑖𝑛 and\n",
    "𝑃 _𝑚𝑎𝑥 arrays\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# dataframe[0] 行數  dataframe[1] 列數\n",
    "# 該屬性中 在 bs set 的最大值 以及 大類中的最小值\n",
    "# range 為兩個相加除以 2\n",
    "# index_split_BS_Majdata 回傳 [0] BS set [1] 回傳 Maj set\n",
    "def range_value(n_nu,sample,sample_class,data):\n",
    "    point_type = check_point_type(n_nu,sample,sample_class,data,False)\n",
    "    index = index_split_BS_Majdata(point_type,sample,sample_class)\n",
    "    BS_max = []\n",
    "    Maj_min = []\n",
    "    all_value = []\n",
    "    for i in range(len(sample[0])): # loop 屬性\n",
    "        for j in range(2): # loop data\n",
    "            if(j==0): # BS set 的 index\n",
    "                BS_index = index[j]\n",
    "                max_value = np.max(sample[BS_index][i]);\n",
    "                BS_max.append(max_value)\n",
    "            else:\n",
    "                Maj_index = index[j]\n",
    "                min_value = np.min(sample[Maj_index][i]);\n",
    "                Maj_min.append(min_value)\n",
    "    # temp = np.array([BS_max,Maj_min]) \n",
    "    range_value = [(BS_max[i] - Maj_min[i])/2 for i in range(len(BS_max))]\n",
    "    all_value = np.array([BS_max,Maj_min,range_value])\n",
    "    return all_value\n",
    "range_value(3,finaldata,output,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b𝑛𝑢𝑚: Number of samples in 𝐵𝑜𝑟𝑑𝑒𝑟 屬於 border 的資料數\n",
    "# compute its(border sample) k nearest neighbors in BS, and save them in BSnn 存放 \n",
    "# compute its(border sample) k nearest neighbors in Maj set, and save them in Majnn\n",
    "def getBSnn_Majnn(n_nu,sample,sample_class,data):     \n",
    "    point_type = check_point_type(n_nu,sample,sample_class,data,False) # 得知所有 sample 中屬於 border 的  \n",
    "    index = index_split_BS_Majdata(point_type,sample,sample_class) \n",
    "    k = knn(n_nu,sample)\n",
    "    BSnn = []\n",
    "    Majnn = []\n",
    "    # 用 border sample 的 index \n",
    "    for i,element in enumerate(point_type):\n",
    "        if(element == \"border\"):\n",
    "            for j in range(2):\n",
    "                for w in index[j]:\n",
    "                    if(j == 0 and i == w ):\n",
    "                        BSnn.append(k[w])\n",
    "                    elif(j==1 and i == w):\n",
    "                        Majnn.append(k[w])\n",
    "        \n",
    "    temp = np.array([BSnn,Majnn]);\n",
    "    print(len(temp[0]+temp[1]))\n",
    "    return temp\n",
    "    \n",
    "len(getBSnn_Majnn(3,finaldata,output,data)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成資料\n",
    "\n",
    "def Populate(r,BSnn,Majnn,Range,P_max,sample,classbelong):\n",
    "    # r 代表要產生的資料的數量 論文定義該數量為增加小類至跟大類相同的數量 達成 50% (大類減小類的數量)\n",
    "    Bl = len(BSnn)-1\n",
    "    nn_nu = len(BSnn[0])-1\n",
    "    Ml = len(Majnn)-1\n",
    "    Synthetic = []\n",
    "    while(r > 0):\n",
    "\n",
    "        s1row = random.randint(0,Bl)\n",
    "        s1col = random.randint(0,nn_nu)\n",
    "        s1 = BSnn[s1row][s1col]\n",
    "        s2row = random.randint(0,Ml)\n",
    "        s2col = random.randint(0,nn_nu)\n",
    "        s2 = Majnn[s2row][s2col]\n",
    "        min_attr = []\n",
    "        for i in range(len(sample[0])):\n",
    "        \n",
    "            if(sample[s1][i] < sample[s2][i]):\n",
    "                min_attr.append(sample[s1][i])\n",
    "            else:\n",
    "                min_attr.append(sample[s2][i])\n",
    "        diff = [(P_max[i] - min_attr[i]) for i in range(len(P_max))]\n",
    "        #print(\"diff\",diff)\n",
    "        gap = random.uniform(0,0.5)\n",
    "        var = [(diff[i] * gap ) for i in range(len(diff))]\n",
    "        temp = []\n",
    "        for i in range(len(sample[0])):\n",
    "            if(min_attr[i]+var[i] <= Range[i]):\n",
    "                temp.append(min_attr[i]+var[i])\n",
    "            else:\n",
    "                temp.append(P_max[i]-var[i])\n",
    "        temp.append(classbelong)\n",
    "        Synthetic.append(temp)   \n",
    "        r = r-1\n",
    "    return Synthetic\n",
    "    \n",
    "needToGenerate = 232\n",
    "BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "Range = range_value(3,finaldata,output,data)\n",
    "P_max = range_value(3,finaldata,output,data)[0]\n",
    "Range = range_value(3,finaldata,output,data)[2]\n",
    "#Populate(needToGenerate,BSnn,Majnn,Range,P_max,finaldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每個類別都要 進行 populate 針對多類別\n",
    "alldata = data.T\n",
    "classCount = classprocess(output)\n",
    "index = 1\n",
    "for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "    if(int(classCount[i][1]) > 0):\n",
    "        over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "        length = alldata.shape[1]\n",
    "        for j in range(len(over)):\n",
    "            alldata[length+j] = over[j]\n",
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = alldata.T.iloc[:,:alldata.shape[0]]\n",
    "y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "overc = Counter(y)\n",
    "for i,v in overc.items():\n",
    "    print(\"class\",i,\"number\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般 SMOTE\n",
    "print(\"SMOTE\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "modelD = DecisionTreeClassifier()\n",
    "modelR = RandomForestClassifier(n_estimators=1000)\n",
    "#clf = clf.fit(X, Y) # \n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresRandomForest = cross_val_score(modelR, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresDecisionTree = cross_val_score(modelD, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCSMOTE\n",
    "print(\"RCSMOTE\")\n",
    "X = alldata.T.iloc[:,:alldata.shape[0]-1]\n",
    "y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "modelD = DecisionTreeClassifier()\n",
    "modelR = RandomForestClassifier(n_estimators=1000)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresRandomForest = cross_val_score(modelR, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresDecisionTree = cross_val_score(modelD, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borderline\n",
    "print(\"Borderline\")\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "stepR = [('border_line', BorderlineSMOTE(random_state=42,kind=\"borderline-1\")), ('model', RandomForestClassifier(n_estimators=1000))] # RandomForestClassifier(n_estimators=1000)\n",
    "stepD = [('border_line', BorderlineSMOTE(random_state=42,kind=\"borderline-1\")), ('model', DecisionTreeClassifier())] \n",
    "pipelineD = Pipeline(steps=stepD)\n",
    "pipelineR = Pipeline(steps=stepR)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresDecisionTree = cross_val_score(pipelineD, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresRandomForest = cross_val_score(pipelineR, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}