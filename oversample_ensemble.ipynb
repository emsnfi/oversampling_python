{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os;\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/emily/Desktop/Research/oversampling_python\"\n",
    "dirs = os.listdir(path+\"/data/pima-5-fold\")\n",
    "os.chdir(path+\"/data/pima-5-fold\")\n",
    "# 到時候 path 這邊也要 loop 不同資料集的資料夾\n",
    "# 输出所有文件和文件夹\n",
    "train = []\n",
    "test = []\n",
    "allfile = []\n",
    "for file in dirs:\n",
    "    if(\"xlsx\" in file):\n",
    "        allfile.append(file)\n",
    "        if(\"tra\" in file ):\n",
    "            train.append(file)\n",
    "        elif(\"tst\" in file):\n",
    "            test.append(file)\n",
    "allfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "191\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "191\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "noisy 231 \n",
      "border 191 \n",
      "safe 193 \n",
      "all 615\n",
      "215\n",
      "400\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "161\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "161\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "noisy 241 \n",
      "border 161 \n",
      "safe 213 \n",
      "all 615\n",
      "215\n",
      "400\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "172\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "172\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "noisy 239 \n",
      "border 172 \n",
      "safe 203 \n",
      "all 614\n",
      "214\n",
      "400\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "181\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "181\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "noisy 238 \n",
      "border 181 \n",
      "safe 195 \n",
      "all 614\n",
      "214\n",
      "400\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "183\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "183\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "noisy 225 \n",
      "border 183 \n",
      "safe 206 \n",
      "all 614\n",
      "214\n",
      "400\n",
      "0.9503267973856209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "import statistics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "os.chdir(\"/Users/emily/Desktop/Research/oversampling_python/data/pima-5-fold\")\n",
    "accuracies=[]\n",
    "oversampleRCSMOTE = [] # 全部 5 個 train data 所訓練出來的 oversample data\n",
    "for i in train:\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    \n",
    "    tempover = []\n",
    "    for i in range(len(classCount)):\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            tempover.extend(over)\n",
    "            \n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "    X = alldata.T.iloc[:,:alldata.shape[0]-1]\n",
    "    y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X, y) # 訓練 model\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    "    \"\"\" 畫樹\n",
    "    dot_data = tree.export_graphviz(clf)\n",
    "    fig = pyplot.figure(figsize=(25,20))\n",
    "    _ = tree.plot_tree(clf)\n",
    "    tree.plot_tree(clf) \n",
    "    \"\"\"\n",
    "    test_file = pd.read_excel(test[i],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(test_data.shape[1]):\n",
    "        test_data.iloc[:,i] = le.fit_transform(test_data.iloc[:,i]) \n",
    "    \"\"\"\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知道 array 的向量維度\n",
    "#np.array().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 選 RCSMOTE 的\n",
    "import random\n",
    "randomRCSMOTE =[]\n",
    "randomindex = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(oversampleRCSMOTE[0],columns= data.columns)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "overSMOTE = []\n",
    "alloverSMOTE = []\n",
    "for i in train:\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "    tempover = []\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    \n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    \n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    \n",
    "    #tempover.extend([X_smote,y_smote])\n",
    "\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    print(len(overSMOTE),\"len\")\n",
    "    alloverSMOTE = [] \n",
    "print(len(overSMOTE[0]))\n",
    "np.array(overSMOTE).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranodom half SMOTE \n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "randomSMOTE = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    tempover = []\n",
    "    over = SMOTE()\n",
    "    X_smote,y_smote = over.fit_resample(finaldata,output)\n",
    "    newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        count = math.floor(int(classCount[i][1])/2); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "\n",
    "    \n",
    "    for index in randomIndex:\n",
    "       \n",
    "        randomtemp.append(overSMOTE[ii][index])\n",
    "        \n",
    "    randomSMOTE.append(randomtemp)\n",
    "    print(ii,\" \",len(randomtemp))\n",
    "    print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "np.array(randomSMOTE).shape\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 random RCSMOTE\n",
    "randomRCSMOTE = []\n",
    "for ii,i in enumerate(train):\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    #data = data.iloc[:,1:]\n",
    "\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "    Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "    Range = range_value(3,finaldata,output,data)\n",
    "    P_max = range_value(3,finaldata,output,data)[0]\n",
    "    Range = range_value(3,finaldata,output,data)[2]\n",
    "    alldata = data.T\n",
    "    classCount = classprocess(output)\n",
    "    randomIndex = []\n",
    "    tempover = []\n",
    "    for j in range(len(classCount)):\n",
    "        if(int(classCount[j][1]) > 0):\n",
    "            over = Populate(int(classCount[j][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[j][0])\n",
    "            tempover.append(over)\n",
    "            #length = alldata.shape[1]\n",
    "    oversampleRCSMOTE.append(tempover)\n",
    " \n",
    "    \n",
    "    for a in range(len(classCount)): # 產生 random index\n",
    "        count = math.ceil(int(classCount[a][1])/2); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(0,len(oversampleRCSMOTE[ii])-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(\"randomindex\",len(randomIndex))\n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(oversampleRCSMOTE[ii][index])\n",
    "    randomRCSMOTE.append(randomtemp)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random SMOTE + RCSMOTE 各一半的結果 \n",
    "allRandomHalf = []\n",
    "temp = []\n",
    "for i in range(len(randomRCSMOTE)):\n",
    "    temp = randomRCSMOTE[i] + randomSMOTE[i]\n",
    "    allRandomHalf.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[17.241222527186938,\n",
       " 144.06180985203403,\n",
       " 67.77905029201011,\n",
       " 10.219548052911474,\n",
       " 16.298417153350194,\n",
       " 37.43439882782506,\n",
       " 13.198113983117274,\n",
       " 36.95063417049988,\n",
       " 'tested_positive']"
      ]
     },
     "metadata": {},
     "execution_count": 465
    }
   ],
   "source": [
    "allRandomHalf[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 513
    }
   ],
   "source": [
    "len(allRandomHalf[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "215\n",
      "400\n",
      "215\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "214\n",
      "400\n",
      "0.9071895424836601\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    for i in range(len(classCount)):\\n    #print(int(classCount[i][1]))\\n        if(int(classCount[i][1]) > 0):\\n            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\\n            length = alldata.shape[1]\\n            for j in range(len(over)):\\n                alldata[length+j] = over[j]\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 536
    }
   ],
   "source": [
    "# 跟原始資料合併\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = np.array(data.iloc[:,l]);\n",
    "    finaldata = np.array(data.iloc[:,:l])\n",
    "    classCount = classprocess(output)\n",
    "    data = data.T\n",
    "    \n",
    "    \n",
    "    for i in range(len(allRandomHalf[index])): # 185\n",
    "        #for j in range(len(allRandomHalf[index][0])): #9\n",
    "            datalength = data.shape[1]\n",
    "            data[datalength+i+1] = allRandomHalf[index][j]\n",
    "    #print(data)  \n",
    "        #print(datalength)\n",
    "        #print(allRandomHalf[index][i],\"sl;d\")\n",
    "    mergeRandom = data.T\n",
    "    l = mergeRandom.shape[1]-1\n",
    "    output = np.array(mergeRandom.iloc[:,l]);\n",
    "    finaldata = np.array(mergeRandom.iloc[:,:l])\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\"\"\"\n",
    "    for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "        if(int(classCount[i][1]) > 0):\n",
    "            #over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "            length = alldata.shape[1]\n",
    "            for j in range(len(over)):\n",
    "                alldata[length+j] = over[j]\n",
    "\"\"\"\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]\n",
    "data = pd.read_excel(train[0],index_col=0)\n",
    "data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "data = pd.DataFrame(data,dtype=object)\n",
    "data = data.iloc[:5,:]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas._testing import assert_frame_equal\n",
    "df3 = pd.DataFrame(overSMOTE[0],columns=data.columns,dtype=object)\n",
    "df3.index += 1\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas._testing import assert_frame_equal\n",
    "df1 = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
    "df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})\n",
    "a = assert_frame_equal(df1, df1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling SMOTE + RCSMOTE  Random 選擇 一半一半\n",
    "# 沒有作用\n",
    "import random \n",
    "import math\n",
    "classCount = classprocess(output)\n",
    "randomSmote = []\n",
    "tempRCSMOTE = []\n",
    "tempSMOTE = []\n",
    "BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "Range = range_value(3,finaldata,output,data)\n",
    "P_max = range_value(3,finaldata,output,data)[0]\n",
    "Range = range_value(3,finaldata,output,data)[2]\n",
    "for i in range(len(classCount)):\n",
    "    count = math.ceil(int(classCount[i][1])/2); # 要產生多少數據  無條件捨去\n",
    "    randomSmote.append([random.randint(0, len(output)-1) for _ in range(count)])\n",
    "print(randomSmote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[1] # 行數    data.shape[0] 列數\n",
    "os.chdir(\"/Users/emily/Desktop/Research/oversampling_python/data/\");\n",
    "#print(os.getcwd());\n",
    "\n",
    "data = pd.read_excel(\"pima.xlsx\")\n",
    "data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "\n",
    "data = data.iloc[:,1:]\n",
    "data.shape[1]\n",
    "l = data.shape[1]-1\n",
    "\n",
    "output = np.array(data.iloc[:,l]);\n",
    "finaldata = np.array(data.iloc[:,:l])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(output)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedata = data\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in range(tedata.shape[1]):\n",
    "    tedata.iloc[:,i] = le.fit_transform(tedata.iloc[:,i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分佈\n",
    "counter = Counter(output)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(output) * 100\n",
    "\tprint(\"class\",k,\"數量：\",v,\"percentage\",'%.3f' %per,\"%\")\n",
    "\t#print('Class=%s, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()\n",
    "ir = 500/268;\n",
    "print(\"IR \",ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出 knn\n",
    "def knn(n_nu,sample):\n",
    "     # n_nu 自己設定多少鄰近個點\n",
    "     # input sample 沒有包含 class label\n",
    "        nnarray = []\n",
    "        neighbors=NearestNeighbors(n_neighbors=n_nu+1).fit(sample)\n",
    "        for i in range(len(sample)):\n",
    "            temp = neighbors.kneighbors(sample[i].reshape(1,-1),return_distance=False)[0]\n",
    "            temp = np.delete(temp,0)\n",
    "            # 有 array 存放 各個點的鄰近點 \n",
    "            nnarray.append(temp)\n",
    "        return nnarray # 回傳相近的點 分別是在第幾個 \n",
    "karray = knn(3,finaldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出大類\n",
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    return  maj\n",
    "\n",
    "find_maj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷該點為什麼類型的點 \n",
    "# 此 index 是 0 開始 所以對照到 excel 的要\n",
    "# inminority 是否是要求在小類中的 如果是則放 true\n",
    "def check_point_type(n_nu,sample,sample_class,data,inminority):\n",
    "    # 使用 find_maj(find_maj(sample_class)\n",
    "    # 使用 knn 找到鄰近的幾個點 \n",
    "    point_type = [] # 放個點是屬於什麼類型的 border safe noisy\n",
    "    n = 0\n",
    "    b = 0\n",
    "    s = 0\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    point = knn(n_nu,sample) # 回傳所有點鄰近 n_nu 個點\n",
    "    #maj = \"\".join(maj) # maj 原本是 list 轉成 str\n",
    "    for index,i in enumerate(point):\n",
    "        maj_nu =0;\n",
    "        if(data.iloc[index,data.shape[1]-1] == maj and inminority == True): # 如果該點本身是大類則不計算\n",
    "            continue; # 如果使用此 則會跟論文的 data 一樣\n",
    "        for j in point[index]: # 每個點鄰近個點的 loop 例如第一個點是 [1,2,3,4,5] 則 loop 裡面的數值\n",
    "            if(data.iloc[j,data.shape[1]-1] == maj ): \n",
    "                maj_nu = maj_nu + 1 # 計算鄰近點為大類的有多少個\n",
    "        if(maj_nu == n_nu ):\n",
    "            point_type.append(\"noisy\");\n",
    "            n = n+1\n",
    "        elif(n_nu/2 < maj_nu and maj_nu < n_nu ):\n",
    "            point_type.append(\"border\");\n",
    "            b = b+1\n",
    "        else:\n",
    "            point_type.append(\"safe\");\n",
    "            s = s+1\n",
    "    print(\"noisy\",n,\"\\nborder\",b,\"\\nsafe\",s,\"\\nall\",n+b+s)\n",
    "    return point_type;\n",
    "point_type = check_point_type(3,finaldata,output,data,True) # 論文使用 knn 為 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集變為小類的 border 跟 safe 以及 大類 去除 小類且為 noisy 的\n",
    "\"\"\"\n",
    "find the maximum value that existed for that attribute among\n",
    "the minority samples of the 𝐵𝑆 set, and also, find the minimum value that existed for\n",
    "the given attribute among the majority samples set\n",
    "\"\"\"\n",
    "# 先是小類去掉 noisy 再來是 大類 \n",
    "def split_BS_Majdata(point_type,sample,sample_class):\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    #maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(sample[i])\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(sample[i])\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "  \n",
    "    return return_sample\n",
    "point_type = check_point_type(3,finaldata,output,data,False)\n",
    "split_BS_Majdata(point_type,finaldata,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_split_BS_Majdata(point_type,sample,sample_class): # maj 以及 BS set 在原本 data 的 index\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # 大類\n",
    "    #maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(i)\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(i)\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "    \n",
    "    return return_sample\n",
    "point_type = check_point_type(3,finaldata,output,data,False)\n",
    "index_split_BS_Majdata(point_type,finaldata,output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Counter(output)\n",
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap\n",
    "\n",
    "classprocess(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The obtained ranges for\n",
    "all features (attributes) are used to control the location of the new synthetic samples in\n",
    "data space.\n",
    "\n",
    "The set of these ranges is denoted by 𝑅𝑎𝑛𝑔𝑒 =\n",
    "(𝑟𝑎𝑛𝑔𝑒1 𝑟𝑎𝑛𝑔𝑒2 𝑟𝑎𝑛𝑔𝑒3 … 𝑟𝑎𝑛𝑔𝑒𝑛𝑎𝑡𝑡𝑟) array where 𝑛𝑎𝑡𝑡𝑟 is the number of\n",
    "attributes in the dataset\n",
    "\n",
    "如何產生 Range array\n",
    "1. we find the maximum value that existed for that attribute among\n",
    "the minority samples of the 𝐵𝑆 set  𝑃 _𝑚𝑎𝑥 = (𝑝𝑚𝑎𝑥1 𝑝𝑚𝑎𝑥2\n",
    "…𝑝𝑚𝑎𝑥𝑛𝑎𝑡𝑡𝑟) array\n",
    "\n",
    "2. find the minimum value that existed for\n",
    "the given attribute among the majority samples set  𝑁 _𝑚𝑖𝑛 = (𝑛𝑚𝑖𝑛1 𝑛𝑚𝑖𝑛2 …𝑛𝑚𝑖𝑛𝑛𝑎𝑡𝑡𝑟)array\n",
    "\n",
    "3. Then, the desired 𝑅𝑎𝑛𝑔𝑒 vector is obtained as the average of 𝑁_𝑚𝑖𝑛 and\n",
    "𝑃 _𝑚𝑎𝑥 arrays\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# dataframe[0] 行數  dataframe[1] 列數\n",
    "# 該屬性中 在 bs set 的最大值 以及 大類中的最小值\n",
    "# range 為兩個相加除以 2\n",
    "# index_split_BS_Majdata 回傳 [0] BS set [1] 回傳 Maj set\n",
    "def range_value(n_nu,sample,sample_class,data):\n",
    "    point_type = check_point_type(n_nu,sample,sample_class,data,False)\n",
    "    index = index_split_BS_Majdata(point_type,sample,sample_class)\n",
    "    BS_max = []\n",
    "    Maj_min = []\n",
    "    all_value = []\n",
    "    for i in range(len(sample[0])): # loop 屬性\n",
    "        for j in range(2): # loop data\n",
    "            if(j==0): # BS set 的 index\n",
    "                BS_index = index[j]\n",
    "                max_value = np.max(sample[BS_index][i]);\n",
    "                BS_max.append(max_value)\n",
    "            else:\n",
    "                Maj_index = index[j]\n",
    "                min_value = np.min(sample[Maj_index][i]);\n",
    "                Maj_min.append(min_value)\n",
    "    # temp = np.array([BS_max,Maj_min]) \n",
    "    range_value = [(BS_max[i] - Maj_min[i])/2 for i in range(len(BS_max))]\n",
    "    all_value = np.array([BS_max,Maj_min,range_value])\n",
    "    return all_value\n",
    "range_value(3,finaldata,output,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b𝑛𝑢𝑚: Number of samples in 𝐵𝑜𝑟𝑑𝑒𝑟 屬於 border 的資料數\n",
    "# compute its(border sample) k nearest neighbors in BS, and save them in BSnn 存放 \n",
    "# compute its(border sample) k nearest neighbors in Maj set, and save them in Majnn\n",
    "def getBSnn_Majnn(n_nu,sample,sample_class,data):     \n",
    "    point_type = check_point_type(n_nu,sample,sample_class,data,False) # 得知所有 sample 中屬於 border 的  \n",
    "    index = index_split_BS_Majdata(point_type,sample,sample_class) \n",
    "    k = knn(n_nu,sample)\n",
    "    BSnn = []\n",
    "    Majnn = []\n",
    "    # 用 border sample 的 index \n",
    "    for i,element in enumerate(point_type):\n",
    "        if(element == \"border\"):\n",
    "            for j in range(2):\n",
    "                for w in index[j]:\n",
    "                    if(j == 0 and i == w ):\n",
    "                        BSnn.append(k[w])\n",
    "                    elif(j==1 and i == w):\n",
    "                        Majnn.append(k[w])\n",
    "        \n",
    "    temp = np.array([BSnn,Majnn]);\n",
    "    print(len(temp[0]+temp[1]))\n",
    "    return temp\n",
    "    \n",
    "len(getBSnn_Majnn(3,finaldata,output,data)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成資料\n",
    "\n",
    "def Populate(r,BSnn,Majnn,Range,P_max,sample,classbelong):\n",
    "    # r 代表要產生的資料的數量 論文定義該數量為增加小類至跟大類相同的數量 達成 50% (大類減小類的數量)\n",
    "    Bl = len(BSnn)-1\n",
    "    nn_nu = len(BSnn[0])-1\n",
    "    Ml = len(Majnn)-1\n",
    "    Synthetic = []\n",
    "    while(r > 0):\n",
    "\n",
    "        s1row = random.randint(0,Bl)\n",
    "        s1col = random.randint(0,nn_nu)\n",
    "        s1 = BSnn[s1row][s1col]\n",
    "        s2row = random.randint(0,Ml)\n",
    "        s2col = random.randint(0,nn_nu)\n",
    "        s2 = Majnn[s2row][s2col]\n",
    "        min_attr = []\n",
    "        for i in range(len(sample[0])):\n",
    "        \n",
    "            if(sample[s1][i] < sample[s2][i]):\n",
    "                min_attr.append(sample[s1][i])\n",
    "            else:\n",
    "                min_attr.append(sample[s2][i])\n",
    "        diff = [(P_max[i] - min_attr[i]) for i in range(len(P_max))]\n",
    "        #print(\"diff\",diff)\n",
    "        gap = random.uniform(0,0.5)\n",
    "        var = [(diff[i] * gap ) for i in range(len(diff))]\n",
    "        temp = []\n",
    "        for i in range(len(sample[0])):\n",
    "            if(min_attr[i]+var[i] <= Range[i]):\n",
    "                temp.append(min_attr[i]+var[i])\n",
    "            else:\n",
    "                temp.append(P_max[i]-var[i])\n",
    "        temp.append(classbelong)\n",
    "        Synthetic.append(temp)   \n",
    "        r = r-1\n",
    "    return Synthetic\n",
    "    \n",
    "needToGenerate = 232\n",
    "BSnn = getBSnn_Majnn(3,finaldata,output,data)[0]\n",
    "Majnn = getBSnn_Majnn(3,finaldata,output,data)[1]\n",
    "Range = range_value(3,finaldata,output,data)\n",
    "P_max = range_value(3,finaldata,output,data)[0]\n",
    "Range = range_value(3,finaldata,output,data)[2]\n",
    "#Populate(needToGenerate,BSnn,Majnn,Range,P_max,finaldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   1                2                3                4    \\\n",
       "Preg              14.0              4.0             15.0              5.0   \n",
       "Plas             175.0            146.0            136.0            116.0   \n",
       "Pres              62.0             78.0             70.0             74.0   \n",
       "Skin              30.0              0.0             32.0             29.0   \n",
       "Insu               0.0              0.0            110.0              0.0   \n",
       "Mass              33.6             38.5             37.1             32.3   \n",
       "Pedi             0.212             0.52            0.153             0.66   \n",
       "Age               38.0             67.0             43.0             35.0   \n",
       "Class  tested_positive  tested_positive  tested_positive  tested_positive   \n",
       "\n",
       "                   5                6                7                8    \\\n",
       "Preg               6.0              3.0              3.0              4.0   \n",
       "Plas               0.0            173.0            162.0            146.0   \n",
       "Pres              68.0             78.0             52.0             92.0   \n",
       "Skin              41.0             39.0             38.0              0.0   \n",
       "Insu               0.0            185.0              0.0              0.0   \n",
       "Mass              39.0             33.8             37.2             31.2   \n",
       "Pedi             0.727             0.97            0.652            0.539   \n",
       "Age               41.0             31.0             24.0             61.0   \n",
       "Class  tested_positive  tested_positive  tested_positive  tested_positive   \n",
       "\n",
       "                   9                10   ...              605  \\\n",
       "Preg               2.0              9.0  ...              1.0   \n",
       "Plas             155.0            145.0  ...            103.0   \n",
       "Pres              52.0             80.0  ...             80.0   \n",
       "Skin              27.0             46.0  ...             11.0   \n",
       "Insu             540.0            130.0  ...             82.0   \n",
       "Mass              38.7             37.9  ...             19.4   \n",
       "Pedi              0.24            0.637  ...            0.491   \n",
       "Age               25.0             40.0  ...             22.0   \n",
       "Class  tested_positive  tested_positive  ...  tested_negative   \n",
       "\n",
       "                   606              607              608              609  \\\n",
       "Preg               2.0              4.0              1.0              2.0   \n",
       "Plas             141.0            147.0            109.0            100.0   \n",
       "Pres              58.0             74.0             60.0             68.0   \n",
       "Skin              34.0             25.0              8.0             25.0   \n",
       "Insu             128.0            293.0            182.0             71.0   \n",
       "Mass              25.4             34.9             25.4             38.5   \n",
       "Pedi             0.699            0.385            0.947            0.324   \n",
       "Age               24.0             30.0             21.0             26.0   \n",
       "Class  tested_negative  tested_negative  tested_negative  tested_negative   \n",
       "\n",
       "                   610              611              612              613  \\\n",
       "Preg               6.0              6.0              5.0              7.0   \n",
       "Plas             114.0             92.0            117.0            119.0   \n",
       "Pres              88.0             92.0             92.0              0.0   \n",
       "Skin               0.0              0.0              0.0              0.0   \n",
       "Insu               0.0              0.0              0.0              0.0   \n",
       "Mass              27.8             19.9             34.1             25.2   \n",
       "Pedi             0.247            0.188            0.337            0.209   \n",
       "Age               66.0             28.0             38.0             37.0   \n",
       "Class  tested_negative  tested_negative  tested_negative  tested_negative   \n",
       "\n",
       "                   614  \n",
       "Preg               1.0  \n",
       "Plas             181.0  \n",
       "Pres              64.0  \n",
       "Skin              30.0  \n",
       "Insu             180.0  \n",
       "Mass              34.1  \n",
       "Pedi             0.328  \n",
       "Age               38.0  \n",
       "Class  tested_positive  \n",
       "\n",
       "[9 rows x 614 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>605</th>\n      <th>606</th>\n      <th>607</th>\n      <th>608</th>\n      <th>609</th>\n      <th>610</th>\n      <th>611</th>\n      <th>612</th>\n      <th>613</th>\n      <th>614</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Preg</th>\n      <td>14.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Plas</th>\n      <td>175.0</td>\n      <td>146.0</td>\n      <td>136.0</td>\n      <td>116.0</td>\n      <td>0.0</td>\n      <td>173.0</td>\n      <td>162.0</td>\n      <td>146.0</td>\n      <td>155.0</td>\n      <td>145.0</td>\n      <td>...</td>\n      <td>103.0</td>\n      <td>141.0</td>\n      <td>147.0</td>\n      <td>109.0</td>\n      <td>100.0</td>\n      <td>114.0</td>\n      <td>92.0</td>\n      <td>117.0</td>\n      <td>119.0</td>\n      <td>181.0</td>\n    </tr>\n    <tr>\n      <th>Pres</th>\n      <td>62.0</td>\n      <td>78.0</td>\n      <td>70.0</td>\n      <td>74.0</td>\n      <td>68.0</td>\n      <td>78.0</td>\n      <td>52.0</td>\n      <td>92.0</td>\n      <td>52.0</td>\n      <td>80.0</td>\n      <td>...</td>\n      <td>80.0</td>\n      <td>58.0</td>\n      <td>74.0</td>\n      <td>60.0</td>\n      <td>68.0</td>\n      <td>88.0</td>\n      <td>92.0</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>Skin</th>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>32.0</td>\n      <td>29.0</td>\n      <td>41.0</td>\n      <td>39.0</td>\n      <td>38.0</td>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>25.0</td>\n      <td>8.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>Insu</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>110.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>185.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>540.0</td>\n      <td>130.0</td>\n      <td>...</td>\n      <td>82.0</td>\n      <td>128.0</td>\n      <td>293.0</td>\n      <td>182.0</td>\n      <td>71.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>180.0</td>\n    </tr>\n    <tr>\n      <th>Mass</th>\n      <td>33.6</td>\n      <td>38.5</td>\n      <td>37.1</td>\n      <td>32.3</td>\n      <td>39.0</td>\n      <td>33.8</td>\n      <td>37.2</td>\n      <td>31.2</td>\n      <td>38.7</td>\n      <td>37.9</td>\n      <td>...</td>\n      <td>19.4</td>\n      <td>25.4</td>\n      <td>34.9</td>\n      <td>25.4</td>\n      <td>38.5</td>\n      <td>27.8</td>\n      <td>19.9</td>\n      <td>34.1</td>\n      <td>25.2</td>\n      <td>34.1</td>\n    </tr>\n    <tr>\n      <th>Pedi</th>\n      <td>0.212</td>\n      <td>0.52</td>\n      <td>0.153</td>\n      <td>0.66</td>\n      <td>0.727</td>\n      <td>0.97</td>\n      <td>0.652</td>\n      <td>0.539</td>\n      <td>0.24</td>\n      <td>0.637</td>\n      <td>...</td>\n      <td>0.491</td>\n      <td>0.699</td>\n      <td>0.385</td>\n      <td>0.947</td>\n      <td>0.324</td>\n      <td>0.247</td>\n      <td>0.188</td>\n      <td>0.337</td>\n      <td>0.209</td>\n      <td>0.328</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>38.0</td>\n      <td>67.0</td>\n      <td>43.0</td>\n      <td>35.0</td>\n      <td>41.0</td>\n      <td>31.0</td>\n      <td>24.0</td>\n      <td>61.0</td>\n      <td>25.0</td>\n      <td>40.0</td>\n      <td>...</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>30.0</td>\n      <td>21.0</td>\n      <td>26.0</td>\n      <td>66.0</td>\n      <td>28.0</td>\n      <td>38.0</td>\n      <td>37.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>Class</th>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>tested_positive</td>\n      <td>...</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_negative</td>\n      <td>tested_positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>9 rows × 614 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 470
    }
   ],
   "source": [
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每個類別都要 進行 populate 針對多類別\n",
    "alldata = data.T\n",
    "classCount = classprocess(output)\n",
    "index = 1\n",
    "for i in range(len(classCount)):\n",
    "    #print(int(classCount[i][1]))\n",
    "    if(int(classCount[i][1]) > 0):\n",
    "        over = Populate(int(classCount[i][1]),BSnn,Majnn,Range,P_max,finaldata,classCount[i][0])\n",
    "        length = alldata.shape[1]\n",
    "        for j in range(len(over)):\n",
    "            alldata[length+j] = over[j]\n",
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = alldata.T.iloc[:,:alldata.shape[0]]\n",
    "y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "overc = Counter(y)\n",
    "for i,v in overc.items():\n",
    "    print(\"class\",i,\"number\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般 SMOTE\n",
    "print(\"SMOTE\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "modelD = DecisionTreeClassifier()\n",
    "modelR = RandomForestClassifier(n_estimators=1000)\n",
    "#clf = clf.fit(X, Y) # \n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresRandomForest = cross_val_score(modelR, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresDecisionTree = cross_val_score(modelD, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCSMOTE\n",
    "print(\"RCSMOTE\")\n",
    "X = alldata.T.iloc[:,:alldata.shape[0]-1]\n",
    "y = alldata.T.iloc[:,alldata.shape[0]-1]\n",
    "modelD = DecisionTreeClassifier()\n",
    "modelR = RandomForestClassifier(n_estimators=1000)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresRandomForest = cross_val_score(modelR, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresDecisionTree = cross_val_score(modelD, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borderline\n",
    "print(\"Borderline\")\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "stepR = [('border_line', BorderlineSMOTE(random_state=42,kind=\"borderline-1\")), ('model', RandomForestClassifier(n_estimators=1000))] # RandomForestClassifier(n_estimators=1000)\n",
    "stepD = [('border_line', BorderlineSMOTE(random_state=42,kind=\"borderline-1\")), ('model', DecisionTreeClassifier())] \n",
    "pipelineD = Pipeline(steps=stepD)\n",
    "pipelineR = Pipeline(steps=stepR)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "scoresDecisionTree = cross_val_score(pipelineD, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "scoresRandomForest = cross_val_score(pipelineR, finaldata, output, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC for RandomForest: %.3f' % mean(scoresRandomForest))\n",
    "print('Mean ROC AUC for DecisionTree: %.3f' % mean(scoresDecisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}