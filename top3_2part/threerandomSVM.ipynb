{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0440c858d28a36c9981deb3f0b3542b13a925970b1503e694b09cf153c81eca91",
   "display_name": "Python 3.7.7  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "440c858d28a36c9981deb3f0b3542b13a925970b1503e694b09cf153c81eca91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.cluster import KMeans  \n",
    "import random\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "path = \"/Users/emily/Desktop/Research/oversampling_python/data/\"\n",
    "folderName = 'yeast3-5-fold' # yeast6-5-fold'#'haberman-5-fold' #'abalone19-5-fold'  pima-5-fold shuttle-c0-vs-c4-5-fold\n",
    "os.chdir(path+ folderName)\n",
    "dirs = os.listdir(path+ folderName)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in dirs:\n",
    "    #print(i.split(\"-\")[-1])\n",
    "    if(\"xlsx\" in i):\n",
    "        if(\"tra\" in i):\n",
    "            train.append(i)\n",
    "\n",
    "        elif(\"tst\" in i):\n",
    "            test.append(i)\n",
    "train = sorted(train)\n",
    "test = sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel(train[0],index_col=0)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    print(maj)\n",
    "    return  maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items(): \n",
    "        #print(className,\" \",number)\n",
    "        print(number)\n",
    "        temp = np.array([className,(maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random polynom_fit_SMOTE \n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "randompolynom = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = data.iloc[:,l];\n",
    "    # 大類比小類多多少數據\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l] # train_x\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    output = le.fit_transform(output) #train_y\n",
    "    tempover = []\n",
    "    over = sv.polynom_fit_SMOTE()\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    X_polynom,y_polynom = over.sample(finaldata,output) # 生成新的 data\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    #print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \"\"\"\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \"\"\"\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) # SMOTE 完後的數據\n",
    "    overpolynom.append(alloverpolynom)\n",
    "    randomIndex = []\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.floor(int(classCount[i][1])*0.8); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_polynom)-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "      randomtemp.append(overpolynom[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randompolynom.append(randomtemp)\n",
    "   \n",
    "np.array(randompolynom).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-31 14:59:43,570:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:44,188:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:44,721:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:45,211:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "1056\n",
      "131\n",
      "2021-05-31 14:59:45,740:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': None}\")\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "131\n",
      "/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/ipykernel_launcher.py:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "# random ProWSyn \n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "randomProWSyn = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    l = data.shape[1]-1\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    \n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    output = le.fit_transform(output)\n",
    "    tempover = []\n",
    "    over = sv.ProWSyn()\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_ProWSyn) - len(data)  # 新生成的 data 數量\n",
    "    #print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \"\"\"\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \"\"\"\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "    randomIndex = []\n",
    "    \n",
    "    for i in range(len(classCount)):\n",
    "        \n",
    "        count = math.floor(int(classCount[i][1])*0.8); # 要產生多少數據  無條件捨去\n",
    "\n",
    "        randomIndex.extend([random.randint(len(data),len(X_ProWSyn)-1) for _ in range(count)]) \n",
    "    # 改改改\n",
    "    randomtemp = []\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "      \n",
    "        \n",
    "        randomtemp.append(overProWSyn[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomProWSyn.append(randomtemp)\n",
    "    #print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomProWSyn).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynom  ProWSyn\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randomProWSyn)):\n",
    "    temp = randompolynom[i] + randomProWSyn[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-31 14:59:47,364:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 14:59:47,365:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 14:59:47,445:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:47,518:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:47,586:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:47,972:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 14:59:47,973:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 14:59:48,059:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:48,135:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:48,207:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:48,547:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 14:59:48,548:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 14:59:48,631:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:48,711:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "130\n",
      "2021-05-31 14:59:48,791:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:49,230:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 14:59:49,231:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 14:59:49,308:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:49,384:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "1056\n",
      "131\n",
      "2021-05-31 14:59:49,459:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:49,803:INFO:SMOTE_IPF: Running sampling via ('SMOTE_IPF', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_folds': 9, 'k': 3, 'p': 0.01, 'voting': 'majority', 'n_jobs': 1, 'classifier': DecisionTreeClassifier(random_state=2), 'random_state': None}\")\n",
      "2021-05-31 14:59:49,805:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': <module 'numpy.random' from '/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/numpy/random/__init__.py'>}\")\n",
      "2021-05-31 14:59:49,888:INFO:SMOTE_IPF: Removing 0 elements\n",
      "2021-05-31 14:59:49,966:INFO:SMOTE_IPF: Removing 0 elements\n",
      "negative\n",
      "negative\n",
      "1057\n",
      "131\n",
      "2021-05-31 14:59:50,043:INFO:SMOTE_IPF: Removing 0 elements\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5, 185, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "# random SMOTE_IPF  \n",
    "alloverSMOTE_IPF  = []\n",
    "overSMOTE_IPF  = []\n",
    "randomSMOTE_IPF  = []\n",
    "\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    lastColumn = data.columns[-1]\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = data.iloc[:,l];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    output = le.fit_transform(output)\n",
    "    tempover = []\n",
    "    over = sv.SMOTE_IPF ()\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    X_SMOTE_IPF ,y_SMOTE_IPF  = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_SMOTE_IPF ) - len(data)  # 新生成的 data 數量\n",
    "    #print(len(X_smote),\"x_smote\")\n",
    "    # 把 X_smote 跟 y_smote 和在一起\n",
    "    \"\"\"\n",
    "    for index,element in enumerate(X_smote):\n",
    "        temp = np.append(element,[y_smote[index]])\n",
    "        alloverSMOTE.append(temp)\n",
    "    overSMOTE.append(alloverSMOTE)\n",
    "    alloverSMOTE =[]\n",
    "    \"\"\"\n",
    "    X_SMOTE_IPF  = pd.DataFrame(X_SMOTE_IPF)\n",
    "    y_SMOTE_IPF  = pd.DataFrame(y_SMOTE_IPF)\n",
    "    alloverSMOTE_IPF  = pd.concat([X_SMOTE_IPF ,y_SMOTE_IPF],axis=1) # SMOTE 完後的數據\n",
    "    overSMOTE_IPF .append(alloverSMOTE_IPF)\n",
    "    randomIndex = []\n",
    "    for i in range(len(classCount)):\n",
    "        #print(classCount[i],\"ffksdl;\")\n",
    "        count = math.floor(int(classCount[i][1])*0.2); # 要產生多少數據  無條件捨去\n",
    "        randomIndex.extend([random.randint(len(data),len(X_SMOTE_IPF )-1) for _ in range(count)]) \n",
    "    \n",
    "    randomtemp = []\n",
    "    #print(overSMOTE)\n",
    "    \n",
    "    \n",
    "    for index in randomIndex:\n",
    "        randomtemp.append(overSMOTE_IPF[ii].iloc[index,:])\n",
    "    #print(randomtemp)\n",
    "   \n",
    "    \n",
    "    randomSMOTE_IPF.append(randomtemp)\n",
    "    #print(ii,\" \",len(randomtemp))\n",
    "    #print(np.array(randomSMOTE).shape)\n",
    "    #print(\"actual\",len(randomSMOTE[ii]))\n",
    "    \n",
    "    #print(\"we\",len(randomSMOTE[ii]))\n",
    "     \n",
    "np.array(randomSMOTE_IPF).shape\n",
    "\n",
    "\n",
    "#現在 randomSMOTE 存的是 random 的 SMOTE 生成 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynom SMOTE_IPF\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randompolynom)):\n",
    "    temp = randompolynom[i] + randomSMOTE_IPF[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProWSyn SMOTE_IPF\n",
    "allRandomHalf = []\n",
    "\n",
    "temp = []\n",
    "for i in range(len(randomProWSyn)):\n",
    "    temp = randomSMOTE_IPF[i] + randomProWSyn[i] # list 合併\n",
    "    temp = np.array(temp)\n",
    "    allRandomHalf.append(temp)\n",
    "\n",
    "for j in range(len(allRandomHalf)):\n",
    "    allRandomHalf[j] = pd.DataFrame(allRandomHalf[j],columns=data.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRandomHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0.0: 1057, 1.0: 1056})\n",
      "Counter({0.0: 1057, 1.0: 1056})\n",
      "Counter({0.0: 1057, 1.0: 1056})\n",
      "Counter({0.0: 1056, 1.0: 1056})\n",
      "Counter({0.0: 1057, 1.0: 1056})\n",
      "0.7137046169239566\n"
     ]
    }
   ],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "accuracies=[]\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    lastColumn = data.columns[-1]\n",
    "\n",
    "    data[lastColumn]= data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allRandomHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    clf=svm.SVC(kernel='rbf',C=1,gamma='auto')\n",
    "    clf = clf.fit(finaldata,output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    \n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "source": [
    "以下為cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 純 polynom_fit_SMOTE\n",
    "\n",
    "alloverSMOTE = []\n",
    "overSMOTE = []\n",
    "accuracies=[]\n",
    "#print(os.getcwd())\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1] -1\n",
    "    output = data.iloc[:,l];\n",
    "    \n",
    "    finaldata = data.iloc[:,:l]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    output = le.fit_transform(output)\n",
    "    \n",
    "    tempover = []\n",
    "    \n",
    "    #over = SMOTE()  # SMOTE\n",
    "    #over = sv.polynom_fit_SMOTE() #  polynom_fit_SMOTE\n",
    "    over = sv.SMOTE_IPF() # SMOTE_IPF()\n",
    "    #over = sv.ProWSyn() # ProWSyn()\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    #X_polynom,y_polynom = over.fit_resample(finaldata,output)\n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_polynom,y_polynom)\n",
    "    #clf = clf.fit(finaldata,output)\n",
    "    #newDataCount = len(X_smote) - len(data)  # 新生成的 data 數量\n",
    "    \n",
    "   \n",
    "    test_file = pd.read_excel(test[ii],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    \"\"\"\n",
    "    # le = preprocessing.LabelEncoder()\n",
    "    # for i in range(test_data.shape[1]):\n",
    "    #     test_data.iloc[:,i] = le.fit_transform(test_data.iloc[:,i]) \n",
    "    \"\"\"\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] # 劃分\n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "   \n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "    \n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Cluster "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "# polynom_fit_SMOTE\n",
    "alloverpolynom = []\n",
    "overpolynom = []\n",
    "\n",
    "centerpolynom = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.polynom_fit_SMOTE()\n",
    "    \n",
    "    X_polynom,y_polynom = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_polynom = pd.DataFrame(X_polynom)\n",
    "    y_polynom = pd.DataFrame(y_polynom)\n",
    "    alloverpolynom = pd.concat([X_polynom,y_polynom],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overpolynom.append(alloverpolynom)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.8); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overpolynom[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerpolynom.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerpolynom[0])) # 第一份資料中的群中心數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerpolynom 只是 index ，value 是取出值\n",
    "centerpolynomvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerpolynom)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerpolynom[i])):\n",
    "            indexpolynom = centerpolynom[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overpolynom[i].iloc[indexpolynom]))\n",
    "        centerpolynomvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "# ProWSyn\n",
    "alloverProWSyn = []\n",
    "overProWSyn = []\n",
    "\n",
    "centerProWSyn = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "    print(\"origin\",Counter(output))\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.ProWSyn()\n",
    "    \n",
    "    X_ProWSyn,y_ProWSyn = over.sample(finaldata,output)\n",
    "    print(Counter(y_ProWSyn))\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_ProWSyn = pd.DataFrame(X_ProWSyn)\n",
    "    y_ProWSyn = pd.DataFrame(y_ProWSyn)\n",
    "    alloverProWSyn = pd.concat([X_ProWSyn,y_ProWSyn],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overProWSyn.append(alloverProWSyn)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.8); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overProWSyn[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerProWSyn.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerProWSyn[0])) # 第一份資料中的群中心數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerProWSyn\n",
    "centerProWSynvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerProWSyn)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerProWSyn[i])):\n",
    "            indexProWSyn = centerProWSyn[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overProWSyn[i].iloc[indexProWSyn]))\n",
    "        centerProWSynvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom + ProWSyn \n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerProWSynvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "# SMOTE-IPF\n",
    "alloverSMOTEIPF = []\n",
    "overSMOTEIPF = []\n",
    "\n",
    "centerSMOTEIPF = []\n",
    "countfor = 0;\n",
    "for ii,i in enumerate(train):\n",
    "    randomIndex = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data = pd.read_excel(i,index_col=0)\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    originlen = data.shape[0]\n",
    "    output = data.iloc[:,data.shape[1]-1];\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:,:data.shape[1]-1]\n",
    "\n",
    "    output = le.fit_transform(output)\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "  #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    tempover = []\n",
    "   \n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    over = sv.SMOTE_IPF()\n",
    "    \n",
    "    X_SMOTEIPF,y_SMOTEIPF = over.sample(finaldata,output)\n",
    "    newDataCount = len(X_polynom) - len(data)  # 新生成的 data 數量\n",
    "    # 把 X_polynom 跟 y_polynom 和在一起\n",
    "    X_SMOTEIPF = pd.DataFrame(X_SMOTEIPF)\n",
    "    y_SMOTEIPF = pd.DataFrame(y_SMOTEIPF)\n",
    "    alloverSMOTEIPF = pd.concat([X_SMOTEIPF,y_SMOTEIPF],axis=1) # SMOTE 完後的數據\n",
    "    \n",
    "    overSMOTEIPF.append(alloverSMOTEIPF)\n",
    "\n",
    "    for i in range(len(classCount)):\n",
    "        countfor = math.floor(int(classCount[i][1])*0.2); # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)]) \n",
    "        \n",
    "        if(countfor>0):\n",
    "            kmeans = KMeans(n_clusters=1)\n",
    "            dtemp = pd.DataFrame(overSMOTEIPF[ii])\n",
    "            X = dtemp.iloc[originlen:,:dtemp.shape[1]-1] # 後來生成的\n",
    "            \n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            \n",
    "            distance = []\n",
    "            X = X.astype('float64')\n",
    "            centers = centers.astype('float64')\n",
    "            tempindata = {}\n",
    "            distancesortemp = []\n",
    "            for i in range(X.shape[0]-1): # 列\n",
    "                \n",
    "                distance = []\n",
    "                temp = 0;\n",
    "                for j in range(X.shape[1]-1):#9 行\n",
    "                    temp = pow((centers[0][j]-X.iloc[i][j]),2)  \n",
    "                    tempindata[i] = temp\n",
    "            \n",
    "            distancesortemp = sorted(tempindata.items(), key=lambda item:item[1])\n",
    "    \n",
    "     \n",
    "            centerSMOTEIPF.append(distancesortemp[:countfor])\n",
    "\n",
    "    \n",
    "\n",
    "print(len(centerSMOTEIPF[0])) # 第一份資料中的群中心數量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centerSMOTEIPF 只是 index ，value 是取出值\n",
    "centerSMOTEIPFvalue =[]\n",
    "for tr in train:\n",
    "    data = pd.read_excel(tr,index_col=0)\n",
    "    originlen = len(data)\n",
    "    for i in range(len(centerSMOTEIPF)):\n",
    "        alltemp = []\n",
    "        for j in range(len(centerSMOTEIPF[i])):\n",
    "            indexSMOTEIPF = centerSMOTEIPF[i][j][0] + originlen\n",
    "            #tempSMOTE = list(overSMOTE[i][indexSMOTE])\n",
    "            alltemp.append(list(overSMOTEIPF[i].iloc[indexSMOTEIPF]))\n",
    "        centerSMOTEIPFvalue.append(alltemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 小類 center polynom-fit-SMOTE + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerpolynom)):\n",
    "    temp = centerpolynomvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合併 小類 center ProWSyn  + SMOTE-IPF\n",
    "\n",
    "allCenterHalf = []\n",
    "temp = []\n",
    "for i in range(len(centerProWSyn )):\n",
    "    temp = centerProWSynvalue[i] + centerSMOTEIPFvalue[i]\n",
    "    temp = pd.DataFrame(temp,columns=data.columns)\n",
    "    allCenterHalf.append(temp)\n",
    "\n",
    "len(allCenterHalf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCenterHalf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 跟原始資料合併\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "mergeRandom = []\n",
    "for index,element in enumerate(train):\n",
    "    data = pd.read_excel(element,index_col =0);\n",
    "    data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.iloc[:,l] = le.fit_transform(data.iloc[:,l])\n",
    "    data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    output = data.iloc[:,l];\n",
    "    finaldata = data.iloc[:,:l]\n",
    "    finaldata.iloc[:,0] = le.fit_transform(finaldata.iloc[:,0])\n",
    "    \"\"\"\n",
    "    #data.iloc[:,0] = le.fit_transform(data.iloc[:,0])\n",
    "    #classCount = classprocess(output)\n",
    "    #data = data.T\n",
    "\n",
    "    #allCenterminHalf[index] = pd.DataFrame(allCenterminHalf[index],columns=data.columns)\n",
    "    mergeRandom = pd.concat([data,allCenterHalf[index]],axis=0)\n",
    "    \n",
    "    finaldata = mergeRandom.iloc[:,:l]\n",
    "    output = mergeRandom.iloc[:,l]\n",
    "    print(Counter(output))\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(finaldata, output)\n",
    "\n",
    "\n",
    "    test_file = pd.read_excel(test[index],index_col=0) #不然會有多出來的 unnamed column\n",
    "    test_data = pd.DataFrame(test_file);\n",
    "    #test_data.Class= test_data.Class.str.replace(\"\\n\", \"\").str.strip()   \n",
    "\n",
    "    test_X = test_data.iloc[:,:(test_data.shape[1])-1] \n",
    "   \n",
    "    test_X.iloc[:,0] = le.fit_transform(test_X.iloc[:,0])\n",
    "    \n",
    "    \n",
    "    #output.iloc[:] = le.fit_transform(output.iloc[:])\n",
    "\n",
    "    test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "    test_y = test_data.iloc[:,test_data.shape[1]-1] \n",
    "\n",
    "    test_y = le.fit_transform(test_y)\n",
    "    test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "    accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "mean = statistics.mean(accuracies)\n",
    "print(mean)\n",
    "\n",
    "#len(mergeRandom[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}