{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os;\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ¯å€‹é»é„°è¿‘çš„ k çš„é» ç”¨ k å€‹é»æ‰€åœ¨çš„å€‹æ•¸ åˆ¤åˆ¥è©²é»(éé„°è¿‘çš„é») å±¬æ–¼ä»€éº¼é¡å‹çš„é» (safe noisy border)\n",
    "# å…ˆç”¨ä¸€å€‹äºŒåˆ†é¡çš„è³‡æ–™\n",
    "\n",
    "# æ­¤ function ç”¨ä¾†åˆ¤åˆ¥æ˜¯å¦ç‚º safe danger(border) or noisy\n",
    "def _in_danger_noise(\n",
    "        self, nn_estimator, samples, target_class, y, kind=\"danger\"\n",
    "    ):\n",
    "        \"\"\"Estimate if a set of sample are in danger or noise.\n",
    "        Used by BorderlineSMOTE and SVMSMOTE.\n",
    "        Parameters\n",
    "        ----------\n",
    "        nn_estimator : estimator object\n",
    "            An estimator that inherits from\n",
    "            :class:`~sklearn.neighbors.base.KNeighborsMixin` use to determine if\n",
    "            a sample is in danger/noise.\n",
    "        samples : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The samples to check if either they are in danger or not.\n",
    "        target_class : int or str\n",
    "            The target corresponding class being over-sampled.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The true label in order to check the neighbour labels.\n",
    "        kind : {'danger', 'noise'}, default='danger'\n",
    "            The type of classification to use. Can be either:\n",
    "            - If 'danger', check if samples are in danger,\n",
    "            - If 'noise', check if samples are noise.\n",
    "        Returns\n",
    "        -------\n",
    "        output : ndarray of shape (n_samples,)\n",
    "            A boolean array where True refer to samples in danger or noise.\n",
    "        \"\"\"\n",
    "        x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]\n",
    "        nn_label = (y[x] != target_class).astype(int)\n",
    "        n_maj = np.sum(nn_label, axis=1)\n",
    "\n",
    "        if kind == \"danger\": \n",
    "            # Samples are in danger / border for m/2 <= m' < m\n",
    "            return np.bitwise_and(\n",
    "                n_maj >= (nn_estimator.n_neighbors - 1) / 2,\n",
    "                n_maj < nn_estimator.n_neighbors - 1,\n",
    "            )\n",
    "        elif kind == \"noise\":\n",
    "            # Samples are noise for m = m'\n",
    "            return n_maj == nn_estimator.n_neighbors - 1\n",
    "        else: # safe sample\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "class Smote:\n",
    "    def __init__(self,samples,N=10,k=5):\n",
    "        self.n_samples,self.n_attrs=samples.shape\n",
    "        self.N=N\n",
    "        self.k=k\n",
    "        self.samples=samples\n",
    "        self.newindex=0\n",
    "       # self.synthetic=np.zeros((self.n_samples*N,self.n_attrs))\n",
    "\n",
    "    def over_sampling(self):\n",
    "        N=int(self.N/100)\n",
    "        self.synthetic = np.zeros((self.n_samples * N, self.n_attrs))\n",
    "        neighbors=NearestNeighbors(n_neighbors=self.k).fit(self.samples)\n",
    "        print 'neighbors',neighbors\n",
    "        for i in range(len(self.samples)):\n",
    "            nnarray=neighbors.kneighbors(self.samples[i].reshape(1,-1),return_distance=False)[0]\n",
    "            #print nnarray\n",
    "            self._populate(N,i,nnarray)\n",
    "        return self.synthetic\n",
    "\n",
    "\n",
    "    # for each minority class samples,choose N of the k nearest neighbors and generate N synthetic samples.\n",
    "    def _populate(self,N,i,nnarray):\n",
    "        for j in range(N):\n",
    "            nn=random.randint(0,self.k-1)\n",
    "            dif=self.samples[nnarray[nn]]-self.samples[i]\n",
    "            gap=random.random()\n",
    "            self.synthetic[self.newindex]=self.samples[i]+gap*dif\n",
    "            self.newindex+=1\n",
    "a=np.array([[1,2,3],[4,5,6],[2,3,1],[2,1,2],[2,3,4],[2,3,4]])\n",
    "s=Smote(a,N=100)\n",
    "print s.over_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output = np.array(data.iloc[:,6)\n",
    "finaldata = np.array(data.iloc[:,:6)\n",
    "\n",
    "output = np.array(data.iloc[:,data.shape[1]-1])\n",
    "finaldata = np.array(data.iloc[:,:data.shape[1]-1]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[1] # è¡Œæ•¸    data.shape[0] åˆ—æ•¸\n",
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier;\n",
    "from sklearn.cluster import MiniBatchKMeans;\n",
    "import collections \n",
    "\n",
    "os.chdir(\"/Users/emily/Desktop/Research/oversampling_python/data\");\n",
    "print(os.getcwd());\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_excel(\"pima.xlsx\")\n",
    "data.Class= data.Class.str.replace(\"\\n\", \"\").str.strip()\n",
    "data = data.iloc[:,1:]\n",
    "data.shape[1]\n",
    "l = data.shape[1]-1\n",
    "\n",
    "output = np.array(data.iloc[:,l]);\n",
    "finaldata = np.array(data.iloc[:,:l])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n",
    "counter = Counter(output)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "finaldata=[]\n",
    "output = []\n",
    "c = np.array(c)\n",
    "import numpy as np \n",
    "for i in range(data.shape[0]):\n",
    "    c = data.T.iloc[:6,i]\n",
    "    c = np.array(c)\n",
    "\n",
    "    finaldata.append(c)\n",
    "finaldata\n",
    "output = np.array(data.T.iloc[:7,:])\n",
    "output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ è‡ªå»º\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "X, y = make_classification(n_classes=3, class_sep=2,\n",
    "                           weights=[0.3,0.5,0.2], n_informative=3, n_redundant=0, flip_y=0,\n",
    "                           n_features=3, n_clusters_per_class=1, n_samples=100, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(1)\n",
    "dict(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors=NearestNeighbors(n_neighbors=6).fit(finaldata)\n",
    "temp = neighbors.kneighbors(finaldata[1].reshape(1,-1),return_distance=False)[0]\n",
    "temp = np.delete(temp,0)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¾å‡º knn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def knn(n_nu,sample):\n",
    "     # n_nu è‡ªå·±è¨­å®šå¤šå°‘é„°è¿‘å€‹é»\n",
    "     # input sample æ²’æœ‰åŒ…å« class label\n",
    "        nnarray = []\n",
    "        neighbors=NearestNeighbors(n_neighbors=n_nu+1).fit(sample)\n",
    "        for i in range(len(sample)):\n",
    "            temp = neighbors.kneighbors(sample[i].reshape(1,-1),return_distance=False)[0]\n",
    "            temp = np.delete(temp,0)\n",
    "            # æœ‰ array å­˜æ”¾ å„å€‹é»çš„é„°è¿‘é» \n",
    "            nnarray.append(temp)\n",
    "        return nnarray # å›å‚³ç›¸è¿‘çš„é» åˆ†åˆ¥æ˜¯åœ¨ç¬¬å¹¾å€‹ \n",
    "karray = knn(5,finaldata)\n",
    "karray[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¾å‡ºå¤§é¡\n",
    "import collections \n",
    "from collections import Counter\n",
    "counter = Counter(output);\n",
    "\n",
    "print(counter)\n",
    "def find_maj(sample_class):\n",
    "    counter = Counter(sample_class);\n",
    "    print(counter)\n",
    "    return  list(dict(counter.most_common(1)).keys())\n",
    "\n",
    "find_maj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# finaldata æ˜¯ä½¿ç”¨ bupa.xlsx çš„\n",
    "# è®Šæˆ class\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "class RCSMOTE(self,n_nu,sample):\n",
    "    def __init__(self):\n",
    "            \n",
    "    def knn(self,n_nu,sample):\n",
    "        self.n_nu = n_nu;\n",
    "        self.sample = sample;\n",
    "        neighbors=NearestNeighbors(n_neighbors=n_nu+1).fit(sample)\n",
    "\n",
    "        nnarray=neighbors.kneighbors(sample[100].reshape(1,-1),return_distance=False)[0]\n",
    "        return nnarray # å›å‚³ç›¸è¿‘çš„é» åˆ†åˆ¥æ˜¯åœ¨ç¬¬å¹¾å€‹ \n",
    "te = RCSMOTE();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(finaldata[:, 0], finaldata[:, 1], c=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ¤æ–·è©²é»ç‚ºä»€éº¼é¡å‹çš„é» å¦‚æœå…¶\n",
    "# æ­¤ index æ˜¯ 0 é–‹å§‹ æ‰€ä»¥å°ç…§åˆ° excel çš„è¦\n",
    "def check_point_type(n_nu,sample,sample_class,data):\n",
    "    # ä½¿ç”¨ find_maj(find_maj(sample_class)\n",
    "    # ä½¿ç”¨ knn æ‰¾åˆ°é„°è¿‘çš„å¹¾å€‹é» \n",
    "    point_type = [] # æ”¾å€‹é»æ˜¯å±¬æ–¼ä»€éº¼é¡å‹çš„ border safe noisy\n",
    "    n = 0\n",
    "    b = 0\n",
    "    s = 0\n",
    "    maj = find_maj(sample_class) # å¤§é¡\n",
    "    point = knn(n_nu,sample) # å›å‚³æ‰€æœ‰é»é„°è¿‘ n_nu å€‹é»\n",
    "    maj = \"\".join(maj) # maj åŸæœ¬æ˜¯ list è½‰æˆ str\n",
    "    for index,i in enumerate(point):\n",
    "        maj_nu =0;\n",
    "        #if(data.iloc[index,data.shape[1]-1] == maj): # å¦‚æœè©²é»æœ¬èº«æ˜¯å¤§é¡å‰‡ä¸è¨ˆç®—\n",
    "            #continue;\n",
    "        for j in point[index]: # æ¯å€‹é»é„°è¿‘å€‹é»çš„ loop ä¾‹å¦‚ç¬¬ä¸€å€‹é»æ˜¯ [1,2,3,4,5] å‰‡ loop è£¡é¢çš„æ•¸å€¼\n",
    "            if(data.iloc[j,data.shape[1]-1] == maj ): \n",
    "                maj_nu = maj_nu + 1 # è¨ˆç®—é„°è¿‘é»ç‚ºå¤§é¡çš„æœ‰å¤šå°‘æ¥µ\n",
    "        if(maj_nu == n_nu ):\n",
    "            point_type.append(\"noisy\");\n",
    "            n = n+1\n",
    "        elif(n_nu/2 < maj_nu and maj_nu < n_nu ):\n",
    "            point_type.append(\"borders\");\n",
    "            b = b+1\n",
    "        else:\n",
    "            point_type.append(\"safe\");\n",
    "            s = s+1\n",
    "    print(\"noisy\",n,\"\\nborder\",b,\"\\nsafe\",s,\"\\nall\",n+b+s)\n",
    "    return point_type;\n",
    "point_type = check_point_type(3,finaldata,output,data) # è«–æ–‡ä½¿ç”¨ knn ç‚º 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è³‡æ–™é›†è®Šç‚ºå°é¡çš„ border è·Ÿ safe ä»¥åŠ å¤§é¡ å»é™¤ å°é¡ä¸”ç‚º noisy çš„\n",
    "\"\"\"\n",
    "find the maximum value that existed for that attribute among\n",
    "the minority samples of the ğµğ‘† set, and also, find the minimum value that existed for\n",
    "the given attribute among the majority samples set\n",
    "\"\"\"\n",
    "# å…ˆæ˜¯å°é¡å»æ‰ noisy å†ä¾†æ˜¯ å¤§é¡ \n",
    "def split_BS_Majdata(point_type,sample,sample_class):\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # å¤§é¡\n",
    "    maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(sample[i])\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(sample[i])\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "    print(BS_sample)\n",
    "    return return_sample\n",
    "\n",
    "print(split_BS_Majdata(point_type,finaldata,output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'tested_negative': 500, 'tested_positive': 268})\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([list([0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 154, 155, 156, 157, 158, 160, 161, 163, 164, 165, 166, 168, 169, 170, 172, 173, 174, 175, 178, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 307, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 332, 333, 384, 385, 386, 387, 388, 389, 390, 391, 394, 395, 396, 397, 398, 400, 401, 403, 404, 405, 406, 408, 409, 460, 461, 462, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 538, 539, 540, 542, 543, 544, 545, 546, 548, 549, 550, 551, 552, 553, 556, 557, 560, 562, 613, 614, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 635, 636, 638, 639, 691, 692, 693, 694, 695, 696, 697, 699, 701, 703, 704, 705, 706, 708, 709, 711, 712, 713, 714, 715, 716, 767]),\n",
       "       list([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766])],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "def index_split_BS_Majdata(point_type,sample,sample_class): # maj ä»¥åŠ BS set åœ¨åŸæœ¬ data çš„ index\n",
    "    BS_sample = []\n",
    "    Maj_sample = []\n",
    "    return_sample = []\n",
    "    maj = find_maj(sample_class) # å¤§é¡\n",
    "    maj = \"\".join(maj)\n",
    "    for i in range(len(sample_class)):\n",
    "        #print(sample_class[i])\n",
    "        \n",
    "        if(sample_class[i] == maj):\n",
    "            Maj_sample.append(i)\n",
    "            \n",
    "        elif(sample_class[i] != maj and point_type[i] != \"noisy\"):\n",
    "            BS_sample.append(i)\n",
    "    return_sample = np.array([ BS_sample ,Maj_sample ])\n",
    "    \n",
    "    return return_sample\n",
    "\n",
    "index_split_BS_Majdata(point_type,finaldata,output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'tested_negative': 500, 'tested_positive': 268})\n",
      "noisy 288 \n",
      "border 225 \n",
      "safe 255 \n",
      "all 768\n",
      "Counter({'tested_negative': 500, 'tested_positive': 268})\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[87.5, 72.778, 67.5015, 83.7, 57.869, 33.841, 92.5, 80.7045]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "\"\"\"\n",
    "The obtained ranges for\n",
    "all features (attributes) are used to control the location of the new synthetic samples in\n",
    "data space.\n",
    "\n",
    "The set of these ranges is denoted by ğ‘…ğ‘ğ‘›ğ‘”ğ‘’ =\n",
    "(ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’1 ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’2 ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’3 â€¦ ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ğ‘›ğ‘ğ‘¡ğ‘¡ğ‘Ÿ) array where ğ‘›ğ‘ğ‘¡ğ‘¡ğ‘Ÿ is the number of\n",
    "attributes in the dataset\n",
    "\n",
    "å¦‚ä½•ç”¢ç”Ÿ Range array\n",
    "1. we find the maximum value that existed for that attribute among\n",
    "the minority samples of the ğµğ‘† set  ğ‘ƒ _ğ‘šğ‘ğ‘¥ = (ğ‘ğ‘šğ‘ğ‘¥1 ğ‘ğ‘šğ‘ğ‘¥2\n",
    "â€¦ğ‘ğ‘šğ‘ğ‘¥ğ‘›ğ‘ğ‘¡ğ‘¡ğ‘Ÿ) array\n",
    "\n",
    "2. find the minimum value that existed for\n",
    "the given attribute among the majority samples set  ğ‘ _ğ‘šğ‘–ğ‘› = (ğ‘›ğ‘šğ‘–ğ‘›1 ğ‘›ğ‘šğ‘–ğ‘›2 â€¦ğ‘›ğ‘šğ‘–ğ‘›ğ‘›ğ‘ğ‘¡ğ‘¡ğ‘Ÿ)array\n",
    "\n",
    "3. Then, the desired ğ‘…ğ‘ğ‘›ğ‘”ğ‘’ vector is obtained as the average of ğ‘_ğ‘šğ‘–ğ‘› and\n",
    "ğ‘ƒ _ğ‘šğ‘ğ‘¥ arrays\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# dataframe[0] è¡Œæ•¸  dataframe[1] åˆ—æ•¸\n",
    "# è©²å±¬æ€§ä¸­ åœ¨ bs set çš„æœ€å¤§å€¼ ä»¥åŠ å¤§é¡ä¸­çš„æœ€å°å€¼\n",
    "# range ç‚ºå…©å€‹ç›¸åŠ é™¤ä»¥ 2\n",
    "# index_split_BS_Majdata å›å‚³ [0] BS set [1] å›å‚³ Maj set\n",
    "def range_value(n_nu,sample,sample_class,data):\n",
    "    point_type = check_point_type(n_nu,sample,sample_class,data)\n",
    "    index = index_split_BS_Majdata(point_type,sample,sample_class)\n",
    "    BS_max = []\n",
    "    Maj_min = []\n",
    "    for i in range(len(sample[0])): # loop å±¬æ€§\n",
    "        for j in range(2): # loop data\n",
    "            if(j==0): # BS set çš„ index\n",
    "                BS_index = index[j]\n",
    "                max_value = np.max(sample[BS_index][i]);\n",
    "                BS_max.append(max_value)\n",
    "            else:\n",
    "                Maj_index = index[j]\n",
    "                min_value = np.min(sample[Maj_index][i]);\n",
    "                Maj_min.append(min_value)\n",
    "    # temp = np.array([BS_max,Maj_min]) \n",
    "    range_value = [(BS_max[i] - Maj_min[i])/2 for i in range(len(BS_max))]\n",
    "    return range_value\n",
    "range_value(3,finaldata,output,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 14.   , 175.   ,  62.   ,  30.   ,   0.   ,  33.6  ,   0.212,\n",
       "        38.   ])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# step1 p.16\n",
    "finaldata[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}