{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emily/Desktop/Research/oversampling_python/FS_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realname = []\n",
    "for dir in os.listdir('../data/all'):\n",
    "    realname.append(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldertemp = []\n",
    "attributes = []\n",
    "for index,element in enumerate(dfa['Name ']):\n",
    "    \n",
    "    if element in realname:\n",
    "        foldertemp.append('../data/all/'+element)\n",
    "    \n",
    "    else:\n",
    "        ll = '../data/all/' + element + '-5-fold'\n",
    "        foldertemp.append(ll)\n",
    "foldertemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/all/dermatology-6/dermatology-6-5-1tra.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/all/dermatology-6-5-fold/dermatology-6-5-1tra.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_excel('./alldataName.xlsx')\n",
    "df = pd.DataFrame(df)\n",
    "dfa = df[df['#Attributes (R/I/N)']>=10]\n",
    "dfa['Name ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['vehicle1','vehicle3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synth(finaldata, output, method):\n",
    "    finaldata = np.array(finaldata)\n",
    "    output = np.array(output)\n",
    "    if method is 'poly':  # \"poly\" in method:\n",
    "        print(\"pol\")\n",
    "        over = sv.polynom_fit_SMOTE()\n",
    "    elif method is 'prow':  # \"proW\" in method:\n",
    "        print(\"pro\")\n",
    "        over = sv.ProWSyn()\n",
    "    elif method is 'SMOTEIPF':  # \"SMOTEIPF\" in method:\n",
    "        print(\"smoteipf\")\n",
    "        over = sv.SMOTE_IPF()\n",
    "    elif method is 'smote':\n",
    "        print(\"smote\")\n",
    "        over = SMOTE(k_neighbors=2)\n",
    "        X_synth, y_syth = over.fit_resample(finaldata, output)\n",
    "        return X_synth, y_syth\n",
    "    elif method is 'baseline':\n",
    "        return finaldata, output\n",
    "\n",
    "    X_synth, y_syth = over.sample(finaldata, output)\n",
    "    return X_synth, y_syth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# 計算要補多少值\n",
    "\n",
    "\n",
    "def find_maj(sample_class):  # 給 class 資料\n",
    "    counter = Counter(sample_class)\n",
    "    maj = list(dict(counter.most_common(1)).keys())\n",
    "    maj = \"\".join(maj)\n",
    "    # print(maj)\n",
    "    return maj\n",
    "\n",
    "\n",
    "def classprocess(output):\n",
    "    c = Counter(output)\n",
    "    datagap = []\n",
    "    maj = find_maj(output)\n",
    "    maj_num = dict(c)[find_maj(output)]\n",
    "    for className, number in c.items():\n",
    "        #print(className,\" \",number)\n",
    "     #   print(number)\n",
    "        temp = np.array([className, (maj_num - number)])\n",
    "        datagap.append(temp)\n",
    "    return datagap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    finaldata represent the X in the data (input atrribute)\n",
    "    output repesent the y in the data (output attribute)\n",
    "    '''\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    lastColumn = data.columns[-1]\n",
    "    data[lastColumn] = data[lastColumn].str.replace(\n",
    "        \"\\n\", \"\").str.strip()\n",
    "    l = data.shape[1]-1\n",
    "    output = data.iloc[:, l]\n",
    "    classCount = classprocess(output)\n",
    "    finaldata = data.iloc[:, :l]\n",
    "    finaldata.iloc[:, 0] = le.fit_transform(\n",
    "        finaldata.iloc[:, 0])\n",
    "    output = le.fit_transform(output)\n",
    "\n",
    "    return classCount, finaldata, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm, ensemble\n",
    "# # 跟原始資料合併 predict with Decision & SVM method\n",
    "\n",
    "\n",
    "def predictDe(train, test, allRandomHalf):\n",
    "    \n",
    "    mergeRandom = []\n",
    "    accuracies = []\n",
    "    for index, element in enumerate(train):\n",
    "        data = pd.read_excel(element, index_col=0)\n",
    "        l = data.shape[1]-1\n",
    "        df = data\n",
    "        finaldata = df.iloc[:, :l]\n",
    "        output = df.iloc[:, l]\n",
    "        feature_count = df.shape[1]-1\n",
    "        # X,y = df.iloc[:,:-1],df.iloc[:,-1]\n",
    "        countforfeature = int(feature_count * 0.8)\n",
    "        chimodel = SelectKBest(score_func=chi2, k=countforfeature)\n",
    "        fit = chimodel.fit(finaldata, output)\n",
    "        # X_new = chimodel.fit_transform(X)\n",
    "        finaldata = fit.transform(finaldata)\n",
    "        finaldata = pd.DataFrame(finaldata)\n",
    "        output.reset_index(inplace=True, drop=True)\n",
    "        print('finaldata:\\n',finaldata)\n",
    "        print('output\\n',output)\n",
    "        print('merge前 con',Counter(data.iloc[:, -1]))\n",
    "        print('final ',finaldata.shape,'output ',output.shape)\n",
    "        data = pd.concat([finaldata,output],axis=1)\n",
    "        print('merge後 con',Counter(data.iloc[:, -1]))\n",
    "        data.columns = allRandomHalf[index].columns\n",
    "        print('mergedata',data.columns) # 15\n",
    "        print('allRadomHalf\\n',allRandomHalf[index].columns)\n",
    "        # print(allRandomHalf.isnull())\n",
    "        lastColumn = data.columns[-1]\n",
    "        \n",
    "        data[lastColumn] = data[lastColumn].str.replace(\"\\n\", \"\").str.strip()\n",
    "        \n",
    "        le = preprocessing.LabelEncoder()\n",
    "        data.iloc[:, -1] = le.fit_transform(data.iloc[:, -1])\n",
    "        data.iloc[:, 0] = le.fit_transform(data.iloc[:, 0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        mergeRandom = pd.concat([data, allRandomHalf[index]], axis=0)\n",
    "        print('mr',mergeRandom.shape)\n",
    "        # finaldata = mergeRandom.iloc[:, :feature_count]\n",
    "        # output = mergeRandom.iloc[:, feature_count]\n",
    "        \n",
    "        \n",
    "        # 把非 numeric 的資料用 label encoder 轉成 numeric 資料\n",
    "        for j in range(mergeRandom.shape[1]):\n",
    "            for k in range(mergeRandom.shape[0]):\n",
    "                # print(df.iloc[j,i])\n",
    "                if isinstance(mergeRandom.iloc[k, j], str):\n",
    "                    mergeRandom.iloc[:, j] = mergeRandom.iloc[:, j].apply(\n",
    "                        lambda col: str(col))\n",
    "                    mergeRandom.iloc[:, j] = le.fit_transform(mergeRandom.iloc[:, j])\n",
    "                    break\n",
    "        # print('allrandom\\n',allRandomHalf.shape)\n",
    "        \n",
    "        finaldata = mergeRandom.iloc[:, :-1]\n",
    "        output = mergeRandom.iloc[:, -1]\n",
    "        print(Counter(output))\n",
    "\n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf = clf.fit(finaldata, output)\n",
    "\n",
    "        # 不然會有多出來的 unnamed column\n",
    "        test_file = pd.read_excel(test[index], index_col=0)\n",
    "        test_data = pd.DataFrame(test_file)\n",
    "        \n",
    "        test_X = test_data.iloc[:, :(test_data.shape[1])-1]\n",
    "        # 把非 numeric 的資料用 label encoder 轉成 numeric 資料\n",
    "        for j in range(test_X.shape[1]):\n",
    "            for k in range(test_X.shape[0]):\n",
    "                # print(df.iloc[j,i])\n",
    "                if isinstance(test_X.iloc[k, j], str):\n",
    "                    test_X.iloc[:, j] = test_X.iloc[:, j].apply(\n",
    "                        lambda col: str(col))\n",
    "                    test_X.iloc[:, j] = le.fit_transform(test_X.iloc[:, j])\n",
    "                    break\n",
    "        test_X.iloc[:, 0] = le.fit_transform(test_X.iloc[:, 0])\n",
    "        test_y = test_data.iloc[:, test_data.shape[1]-1]\n",
    "        \n",
    "        fit = chimodel.fit(test_X,test_y)\n",
    "        # X_new = chimodel.fit_transform(X)\n",
    "        test_X = fit.transform(test_X)\n",
    "        \n",
    "        test_y_predicted = clf.predict(test_X)\n",
    "\n",
    "        \n",
    "\n",
    "        test_y = le.fit_transform(test_y)\n",
    "        test_y_predicted = le.fit_transform(test_y_predicted)\n",
    "\n",
    "        accuracy = roc_auc_score(test_y, test_y_predicted)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    mean = statistics.mean(accuracies)\n",
    "    mean = statistics.mean(accuracies)\n",
    "    meanRound = round(mean, 3)\n",
    "    print(meanRound)\n",
    "    return meanRound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "from numpy.core.fromnumeric import size\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "from sklearn import tree\n",
    "\n",
    "from itertools import permutations\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm, ensemble\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import Counter\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as pl\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm, ensemble\n",
    "from imblearn.over_sampling._smote.base import SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import smote_variants as sv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1 = list(permutations(\"442\", 3))\n",
    "f2 = list(permutations(\"253\", 3))\n",
    "f3 = list(permutations(\"334\", 3))\n",
    "\n",
    "f = f1+f2+f3\n",
    "temp = []\n",
    "    # get rid of the repeat ratio composition\n",
    "for i in f:\n",
    "    if i not in temp:\n",
    "        temp.append(i)\n",
    "f = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HERE\n",
    "def ElbowCenterGenerate(train, ratio, method, path):\n",
    "    if ratio == 0:\n",
    "        return []\n",
    "    alloverpolynom = []\n",
    "    overpolynom = []\n",
    "    centerpolynom = []\n",
    "    centerpolynomvalue = []\n",
    "    countfor = 0\n",
    "    for ii, i in enumerate(train):\n",
    "        print(\"第幾個\", i)\n",
    "        data = pd.read_excel(i, index_col=0)\n",
    "        #print(i, \"traindata1\", data)\n",
    "        classCount, finaldata, output = preprocess(data)\n",
    "        \n",
    "        # 進行 feature selection filter chi-square\n",
    "        df = data\n",
    "        feature_count = df.shape[1]-1\n",
    "        # X,y = df.iloc[:,:-1],df.iloc[:,-1]\n",
    "        print('總共有：',feature_count)\n",
    "        countforfeature = int(feature_count * 0.8)\n",
    "        print('要篩出：',countforfeature)\n",
    "        \n",
    "        X = finaldata\n",
    "        chimodel = SelectKBest(score_func=chi2, k=countforfeature)#)\n",
    "        # fitmodel = chimodel.fit(finaldata, output)\n",
    "        # X_new = chimodel.fit_transform(X)\n",
    "        finaldata = chimodel.fit_transform(finaldata,output)\n",
    "        finaldata = pd.DataFrame(finaldata)\n",
    "        \n",
    "        # 找出挑到的 feature\n",
    "        scores = chimodel.scores_\n",
    "        # print('model scores:', scores) #得分越高,特征越重要\n",
    "        p_values = chimodel.pvalues_\n",
    "        # print('model p-values', p_values)#p-values越小,置信度越高,特征越重要\n",
    "        #按重要性排序,选出最重要的k个\n",
    "        indices = np.argsort(scores)[: :-1]\n",
    "        # print('in',indices)\n",
    "        k_best_features = list(X.columns.values [indices[0:countforfeature]])\n",
    "        # print('k best features are: ',k_best_features)\n",
    "        allcolumns = k_best_features\n",
    "        allcolumns.append(data.columns[-1])\n",
    "        # print('allcol:',allcolumns)\n",
    "        # 把非 numeric 的資料用 label encoder 轉成 numeric 資料\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        # for col in range(finaldata.shape[1]-1):\n",
    "        #     if isinstance(finaldata.iloc[0, :][col], str):\n",
    "        #         finaldata.iloc[:, col] = le.fit_transform(\n",
    "        #             finaldata.iloc[:, col])\n",
    "\n",
    "        for j in range(finaldata.shape[1]):\n",
    "            for k in range(finaldata.shape[0]):\n",
    "                # print(df.iloc[j,i])\n",
    "                if isinstance(finaldata.iloc[k, j], str):\n",
    "                    finaldata.iloc[:, j] = finaldata.iloc[:, j].apply(\n",
    "                        lambda col: str(col))\n",
    "                    finaldata.iloc[:, j] = le.fit_transform(\n",
    "                        finaldata.iloc[:, j])\n",
    "                    break\n",
    "        print(Counter(output))\n",
    "        originlen = data.shape[0]  # 原始的 data 數量\n",
    "        X_polynom, y_polynom = synth(\n",
    "            finaldata, output, method)\n",
    "        X_polynom = pd.DataFrame(X_polynom)\n",
    "        y_polynom = pd.DataFrame(y_polynom)\n",
    "        alloverpolynom = pd.concat(\n",
    "            [X_polynom, y_polynom], axis=1)  # SMOTE 完後的數據\n",
    "        overpolynom.append(alloverpolynom)\n",
    "        tempcenterpolynom = []\n",
    "        for i in range(len(classCount)):  # 不同類個別要產生多少數據才能平衡 目前是二分類\n",
    "            origincount = int(classCount[i][1])\n",
    "            print(\"要產生的資料數\", origincount)\n",
    "            countfor = math.floor(\n",
    "                int(classCount[i][1])*ratio)  # 要產生多少數據  無條件捨去\n",
    "        #randomIndex.extend([random.randint(len(data),len(X_smote)-1) for _ in range(count)])\n",
    "\n",
    "            if(countfor > 0):\n",
    "                dtemp = pd.DataFrame(overpolynom[ii])\n",
    "                X = dtemp.iloc[originlen:, :dtemp.shape[1]-1]  # 後來生成的 都是小類\n",
    "                X.reset_index(inplace=True, drop=True)\n",
    "            # print(\"要產生多少\",countfor)\n",
    "            # 計算應該分成幾群\n",
    "                model = KMeans()\n",
    "                visualizer = KElbowVisualizer(model, k=(1, 12))\n",
    "\n",
    "                # Fit the data to the visualizer\n",
    "                kmodel = visualizer.fit(X)\n",
    "                cluster_count = kmodel.elbow_value_  # 最佳要分成幾群\n",
    "                kmeans = KMeans(n_clusters=cluster_count)\n",
    "                kmeans.fit(X)\n",
    "                label = Counter(kmeans.labels_)  # 標籤分類狀況\n",
    "\n",
    "                # 不同群的比例\n",
    "                labelRatio = []\n",
    "                for key, element in sorted(label.items()):\n",
    "                    labelRatio.append(element/origincount)\n",
    "            # print(labelRatio)\n",
    "\n",
    "            # 把分類標籤跟原始資料進行合併\n",
    "                klabel = pd.DataFrame(\n",
    "                    {'label': kmeans.labels_})  # 建立一個欄位名為 label 的\n",
    "                df = pd.concat([X, klabel], axis=1)  # X 是後來生成的數據 類別都是小類\n",
    "            # print(df)\n",
    "                centers = kmeans.cluster_centers_  # 各群群中心\n",
    "\n",
    "                distance = []\n",
    "                X = X.astype('float64')\n",
    "                centers = centers.astype('float64')\n",
    "                tempindata = {}\n",
    "                distancesortemp = []\n",
    "\n",
    "            # 計算每個點跟各群中心的距離\n",
    "\n",
    "                ct = 0\n",
    "            # print(\"分成\",cluster_count,\"群\")\n",
    "            # print(\"要產生\",countfor)\n",
    "                tempcenterpolynom = []  # 清空\n",
    "                for ic in range(cluster_count):\n",
    "                    ct += 1\n",
    "\n",
    "                    temppolynom = []\n",
    "                # 把不同群過濾出來\n",
    "                    # df 是 X 跟 label 結合後的 dataframe\n",
    "                    tempdf = df[df['label'] == ic]\n",
    "                # allCluster.append(df[df['label']==ic])\n",
    "\n",
    "                # 計算每個點跟群中心的距離\n",
    "                    for i in range(tempdf.shape[0]-1):  # 列 也就是幾筆資料\n",
    "\n",
    "                        distance = []\n",
    "                        temp = 0  # 放算出來的距離\n",
    "                        tempsum = 0\n",
    "                        # 到前一欄 因為最後一欄為 label\n",
    "                        for j in range(tempdf.shape[1]-2):\n",
    "                            # 該欄位跟center欄位的距離\n",
    "                            temp = pow((centers[ic][j]-tempdf.iloc[i][j]), 2)\n",
    "                            tempsum = tempsum + temp\n",
    "                        # print(tempsum)\n",
    "                            tempindata[i] = tempsum\n",
    "\n",
    "                    distancesortemp = sorted(\n",
    "                        tempindata.items(), key=lambda item: item[1])\n",
    "                # print(distancesortemp)\n",
    "\n",
    "                # 要按照比例挑出資料\n",
    "\n",
    "                    countforlabel = math.ceil(\n",
    "                        countfor * labelRatio[ic])  # 按照比例 給不同的數量 不同群不同數量\n",
    "                # print(\"比例\",labelRatio)\n",
    "                    temppolynom.extend(\n",
    "                        distancesortemp[:countforlabel])  # 該群所要的數量\n",
    "                # print(\"該群所要的數量\",len(temppolynom))\n",
    "            # tempcenterpolynom.extend(temppolynom) # 該份資料集所要的所有資料\n",
    "\n",
    "                # print(\"ct\",ct)\n",
    "                    tempcenterpolynom = tempcenterpolynom+temppolynom\n",
    "\n",
    "                centerpolynom.append(tempcenterpolynom)  # 所有資料集所選到的資料\n",
    "            # print(\"真的有幾筆\",len(centerpolynom[ii]))\n",
    "        # print(centerpolynom[0])\n",
    "        for i in range(len(centerpolynom)):\n",
    "            alltemp = []\n",
    "            for j in range(len(centerpolynom[i])):\n",
    "                indexpolynom = centerpolynom[i][j][0] + originlen - 1\n",
    "                alltemp.append(list(overpolynom[i].iloc[indexpolynom]))\n",
    "            centerpolynomvalue.append(alltemp)\n",
    "    \n",
    "    return centerpolynomvalue,allcolumns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the feature selection elbow center function\n",
    "# def calculatethreemethod(train, test, id, path, approach, maxsingle, sheetName):\n",
    "def calculatethreemethod(train, test, id, path, approach, sheetName):\n",
    "    '''\n",
    "    1. train is assign by train data\n",
    "    2. test\n",
    "    3. approach means the choice of random or center\n",
    "    4. id is for the purpose of writing into the excel different\n",
    "        dataset need to be on different row\n",
    "    5. path right now is useless\n",
    "    '''\n",
    "    cell = 1\n",
    "    \n",
    "    originpath = os.getcwd() \n",
    "    os.chdir(path)\n",
    "    tempfileName = train[0]  # 要擷取 file 的名字\n",
    "    tempfileName = tempfileName.split('-')\n",
    "    fileName = '-'.join(tempfileName[:-2])\n",
    "    data = pd.read_excel(train[0], index_col=0)\n",
    "    # print(\"dsdsadad\", data.columns)\n",
    "    tempappr = []  # 放要比較的數值(3,3,4 之間會比較)\n",
    "    randompoly = []\n",
    "    randomPro = []\n",
    "    randomIPF = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for index in range(len(f)):\n",
    "        '''\n",
    "        if approach == 'random':\n",
    "            randompoly = RandomCal.RandomGenerate(\n",
    "                train, int(f[index][0])*0.1, \"poly\", path)\n",
    "            randomPro = RandomCal.RandomGenerate(\n",
    "                train, int(f[index][1])*0.1, \"prow\", path)\n",
    "            randomIPF = RandomCal.RandomGenerate(\n",
    "                train, int(f[index][2])*0.1, \"SMOTEIPF\", path)\n",
    "        elif approach == 'center':\n",
    "            randompoly = RandomCal.CenterGenerate(\n",
    "                train, int(f[index][0])*0.1, \"poly\", path)\n",
    "            randomPro = RandomCal.CenterGenerate(\n",
    "                train, int(f[index][1])*0.1, \"prow\", path)\n",
    "            randomIPF = RandomCal.CenterGenerate(\n",
    "                train, int(f[index][2])*0.1, \"SMOTEIPF\", path)\n",
    "        elif approach == 'elbowRandom':\n",
    "            randompoly = RandomCal.ElbowRandomGenerate(\n",
    "                train, int(f[index][0])*0.1, \"poly\", path)\n",
    "            randomPro = RandomCal.ElbowRandomGenerate(\n",
    "                train, int(f[index][1])*0.1, \"prow\", path)\n",
    "            randomIPF = RandomCal.ElbowRandomGenerate(\n",
    "                train, int(f[index][2])*0.1, \"SMOTEIPF\", path)\n",
    "        elif approach == 'elbowCenter':\n",
    "            randompoly = RandomCal.ElbowCenterGenerate(\n",
    "                train, int(f[index][0])*0.1, \"poly\", path)\n",
    "            randomPro = RandomCal.ElbowCenterGenerate(\n",
    "                train, int(f[index][1])*0.1, \"prow\", path)\n",
    "            randomIPF = RandomCal.ElbowCenterGenerate(\n",
    "                train, int(f[index][2])*0.1, \"SMOTEIPF\", path)\n",
    "        '''\n",
    "        \n",
    "        randompoly,allcolumns = ElbowCenterGenerate(\n",
    "                train, int(f[index][0])*0.1, \"poly\", path)\n",
    "        \n",
    "        randomPro = ElbowCenterGenerate(\n",
    "                train, int(f[index][1])*0.1, \"prow\", path)[0]\n",
    "        randomIPF = ElbowCenterGenerate(\n",
    "                train, int(f[index][2])*0.1, \"SMOTEIPF\", path)[0]\n",
    "        \n",
    "        # start = datetime.datetime.now()\n",
    "        start = time.process_time()\n",
    "\n",
    "        allRandom = []\n",
    "        temp = []\n",
    "        # print('elbow',randomPro.shape)\n",
    "        for i in range(len(randomPro)):\n",
    "            temp = randomIPF[i] + randomPro[i]  # list 合併\n",
    "            temp = temp + randompoly[i]\n",
    "            temp = np.array(temp)\n",
    "            allRandom.append(temp)\n",
    "        \n",
    "        print('feature:',allcolumns)  \n",
    "     \n",
    "        for j in range(len(allRandom)):  #\n",
    "            allRandom[j] = pd.DataFrame(allRandom[j],columns= allcolumns)\n",
    "        print('following123\\n',allRandom[0].shape)\n",
    "       \n",
    "        cell = cell + 1\n",
    "        # 為了算個別 C4.5 跟 SVM 時間 所以先註解掉 SVM 的部分\n",
    "        \n",
    "        meanDe = predictDe(train, test, allRandom)\n",
    "        # meanSVM = CA.predictSVM(train, test, allRandom)\n",
    "        end = time.process_time()\n",
    "\n",
    "        # write in excel prepare\n",
    "        # 時間\n",
    "        dur = end - start\n",
    "        dur = round(dur, 3)\n",
    "        \n",
    "        \n",
    "        print('into the file')\n",
    "        wb = load_workbook('/Users/emily/Desktop/Research/FS_ensemble.xlsx')\n",
    "        # sheet = wb['Ensemble 三個方法 Random']\n",
    "        # sheet = wb['Ensemble 三個方法 ElbowCenter']\n",
    "        # sheet Name\n",
    "        sheet = wb[sheetName]\n",
    "        print(fileName)\n",
    "        rr =0\n",
    "        # rr = id +1\n",
    "        sheet.cell(row=1, column=1, value=fileName)\n",
    "        sheet.cell(row=1, column=cell, value=meanDe)\n",
    "        # 時間\n",
    "        # rr = id +2\n",
    "        sheet.cell(row=2, column=cell, value=dur)\n",
    "        # sheet.cell(row=3+id, column=cell, value=meanSVM)\n",
    "\n",
    "        # 找最大值 並上紅色\n",
    "        '''\n",
    "        ma = meanDe\n",
    "        # ma = meanSVM\n",
    "        tempappr.append(ma)\n",
    "\n",
    "        fontRed = Font(color='FF0000', size=16)  # point at red font\n",
    "        fontBoldRed = Font(color='FF0000', bold=True, size=16)\n",
    "        if ma > maxsingle:\n",
    "\n",
    "            sheet.cell(1+id, cell).font = fontBoldRed  # C4.5\n",
    "            # sheet.cell(3+id, cell).font = fontBoldRed  # SVM\n",
    "\n",
    "            print('meanDe233', ma)\n",
    "        if index == 2 or index == 9 or index == 12:\n",
    "            maxcell = max(tempappr)  # 每個方法的最大值\n",
    "            if maxcell < maxsingle:\n",
    "                print(\"new round3\", tempappr)\n",
    "                maxindex = tempappr.index(maxcell)  # 最大值的 index\n",
    "            # print('maxindex', maxindex)\n",
    "                # r = 3+id  # SVM\n",
    "                r = 1 + id  # C4.5\n",
    "                c = cell - (len(tempappr) - maxindex) + 1\n",
    "                print('row3', r, '  column4', c)\n",
    "                sheet.cell(r, c).font = fontRed\n",
    "\n",
    "                print('meanDe111', maxcell)\n",
    "            tempappr = []\n",
    "        # 若大於 singel method 則標粗體\n",
    "        '''\n",
    "        wb.save('/Users/emily/Desktop/Research/test2-1.xlsx')\n",
    "    \n",
    "        \n",
    "    os.chdir(originpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder process\n",
    "def train_test_split(folder):\n",
    "    train = []\n",
    "    test = []\n",
    "    # os.chdir(folder)\n",
    "    dirs = os.listdir(folder)\n",
    "    for i in dirs:\n",
    "        # print(i.split(\"-\")[-1])\n",
    "        if(\"xlsx\" in i):\n",
    "            if(\"tra\" in i):\n",
    "                train.append(i)\n",
    "\n",
    "            elif(\"tst\" in i):\n",
    "                test.append(i)\n",
    "    train = sorted(train)\n",
    "    test = sorted(test)\n",
    "    return train, test\n",
    "folderpath = []\n",
    "for i in dfa['Name ']:\n",
    "    tmp = '../data/all/' + i\n",
    "    folderpath.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emily/Desktop/Research/oversampling_python/FS_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folderpath:\n",
    "    path = folder\n",
    "    \n",
    "    train, test = train_test_split(folder)\n",
    "    column = 2\n",
    "    calculatethreemethod(train, test, id, path, 'elbowCenter',\n",
    "                            'Ensemble 兩個方法 ElbowCenter')\n",
    "    id = id+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRandomHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO\n",
    "for element in train:\n",
    "    df = pd.read_excel(element,index_col=0)\n",
    "    feature_count = df.shape[1]-1\n",
    "    X,y = df.iloc[:,:-1],df.iloc[:,-1]\n",
    "    countforfeature = int(feature_count * 0.8)\n",
    "    chimodel = SelectKBest(score_func=chi2, k=countforfeature)\n",
    "    fit = chimodel.fit(X, y)\n",
    "    # X_new = chimodel.fit_transform(X)\n",
    "    X_new = chimodel.transform(X)\n",
    "    X_new = pd.DataFrame(X_new)\n",
    "    df_new = pd.concat([X_new,y],)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculatethreemethod(train, test, id, path, 'elbowRandom',\n",
    "    #                          maxsingle, 'Ensemble 兩個方法 ElbowRandom')\n",
    "    # y_new = chimodel.transform(y)\n",
    "    print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 feature selection 後的資料丟進 oversampling ensemble 中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/users/emily/desktop/research/oversampling_python/fs_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel('../data/all/vehicle2/vehicle2-5-5tra.xlsx',index_col=0)\n",
    "X,y = d.iloc[:,:-1],d.iloc[:,-1]\n",
    "model = SelectKBest(chi2, k=3).fit(X,y)\n",
    "\n",
    "# X_new = SelectKBest(chi2, k=15).fit_transform(X, y)\n",
    "X_new = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.scores_\n",
    "print('model scores:', scores) #得分越高,特征越重要\n",
    "p_values = model.pvalues_\n",
    "print('model p-values', p_values)#p-values越小,置信度越高,特征越重要\n",
    "#按重要性排序,选出最重要的k个\n",
    "indices = np.argsort(scores)[: :-1]\n",
    "k_best_features = list(X.columns.values[indices[0:15]])\n",
    "print('k best features are: ',k_best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emily/Desktop/Research/oversampling_python/venv/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from Py_FS.wrapper.nature_inspired import GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GA() missing 1 required positional argument: 'train_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2651d722db7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: GA() missing 1 required positional argument: 'train_label'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_iris()\n",
    "data.target\n",
    "data = datasets.load_iris()\n",
    "d = GA(20,100,data.data,data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.final_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "140ba02d0dd32d113ccd698594a4c9ecefe41d39bf7db07a36a271a926c6d995"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
